1.NoSQL 数据库

严格一致性：数据的变化是源自的，一经改变
顺序一致性：每个客户端看到的数据依照它们操作执行的顺序而变化
因果一致性：客户端以因果关系顺序观察到数据的改变
最终一致性：在没有更新数据的一段时间里，系统将通过广播保证副本之间的数据一致性
弱一致性：没有做出保证的情况下，所有的更新会通过广播的形式传递，展现给不同客户端的数据顺序可能不一样。

一个分布式系统只能同时实现一致性、可用性和分区容错性中的两个
放宽一致性来提高系统可用性是一个非常有效的提议，不过这种方案会强制让应用层
去解决一致性问题，因此也会增加系统的复杂度

1.3.1 维度
数据模型：
数据有多重存储的方式，包括键/值对（类似于HashMap）、半结构化的列式存储和文档结构存储。用户的应用如何存取数据？同时数据模式是否随着时间而变化

存储模型：
内存还是持久化？RDBMS通常持久化存储数据到磁盘中。即使需要的是纯粹的内存模式，也仍旧有其他方案。一旦考虑持久化存储，就需要考虑选择的方案是否会影响到访问模式？

一致性模型：
严格一致性几路车最终一致性？问题是存储系统如何实现它的目标：必须降低一致性要求?

物理模型：
分布式模式还是单机模式？这种架构看起来像什么？是仅仅运行在单个及其上，还是分布在多台机器上，但分布及扩展规则由客户端管理，换句话说，由用户自己的代码管理？也许分布式模式仅仅是个事后的工作，并且只会在用户。

读写性能
用户必须了解自己的应用程序的访问模式。是读多写少？还是读写相当或者写多读少？是用范围扫描数据好，还是随机读请求数据更好，有些系统仅对这些情况中的一种支持得很好，有些系统则对各种情况都提供了支持。

辅助索引
辅助索引支持用户按不同字段和顺序方式来访问表。这个维度覆盖了某些完全没有辅助索引支持且不保证数据排序的系统

故障处理：
及其

压缩：

负载均衡：

原子操作的读-修改-写

加锁、等待、死锁

1.3.2 可扩展性

RDBMS非常适合事务性操作，但不见长于超大规模的数据分析处理，分析型数据库可以存储数百或数千TB的数据，在一台服务器上做查询工作的响应时间，会远远超过用户可接受的合理响应时间。垂直扩展服务器性能，即增加CPU核数和磁盘数目，也
RDBMS的等待和死锁的出现频率，与事务和并发的增加并不是线性关系，准确的说，与并发数目的平方以及事务规模的3次方甚至5次方相关。分区通常是一个不切实际的方案，

1.3.3 数据库的范式化和反范式化

不同规模需要设计不同的系统结构，最佳描述是：反范式化、复制、和智能主键，
部分原则是采用反范式化模式，将数据复制到多张表中，这样在读取时就不需从多张表中聚合数据了。或者预先物化所需的视图，一次优化从而避免进一步的处理来提高读取性能。
短网址存储在shorturl表中，用户可以点击短网址来链接到完整网址。系统会跟踪每次点击，记录该网址的使用次数，还会记录一些其他信息，。
用户信息存储在user表中，用户可以在Hush网站上注册并创建个人短网址列表，同时也可以在此网站上增加描述。user表与shorturl表之间维护了一个外键关系。
系统还会在后台下载链接到的页面，并提取一些TITLE之类的HTML标签。将整个页面保存下来的目的是，供后续的异步任务进行处理和分析。这些内容都由url表存储。
每个链接页面只存储一份，不过，由于许多用户可能会链接同一个长网址，并且还想保存他们自己的详细信息，例如，
这使得通过统计原始的短网址标识 refShortId，就可以统计任意一个短网址映射到同一个长网址的使用率。ShortId 和 refShortId 利用散列ID的方式被唯一地分配给了短网址。

每个短网址都存储在独立的表shorturl 中，表中还包含了使用统计信息，按统计时间范围不同，存放于不同的列族中，同时每个值都有其生存期。列名是日期和一个可选维度后缀的组合，例如国家代码，列值则是对应计数器的值。
下载下来的页面和提取的详细信息存储在url表中，并且要通过压缩最大限度地减少存储量，因为存储的页面主要是HTML，这种格式本身冗余量大，并且包含了大量文本。
系统通过user-shorturl 表可以快速查到指定用户的所有短网址标识。这个功能被用在用户主页中，只要用户一登录就会被记录下来。 user 表存储着实际用户的详细信息。
虽然表的数量相同，都是4个，但表的含义发生了变化：clicks 表被合并到 shorturl表中，统计列使用日期为列键，格式为YYYYMMDD，例如，20110502，这样用户可以顺序访问数据。新增的 user-shorturl 表代替了外键，使得查询用户相关信息变得快捷。
对洗漱矩阵、宽表、列式存储的支持使得数据在存储的时候无需范式化，同时也可以避免查询时采用开销很大的JOIN操作聚合数据。使用智能主键可以控制数据怎样去存储以及存储在什么位置。由于可以使用行键的部分内容进行范围检索，行键作为组合件设计时，与字典序左部分为头的索引效果相似。因此，正确的设计能够使性能不会因为数据增长而下降，例如当数据条目从10条增加到1000万条时，系统仍旧可以保持相同的读写性能。

1.4 结构
摒弃关系型的特点，采用简单的API来进行增删改查，再加一个扫描函数，在交大的键范围或权标范围上迭代扫描，

1.4.2 表、行、列和单元格

最基本的单位是列，一列或多列形成一行，并由唯一的行键来确定存储。反过来，一个表中有若干航，其中每列可能有多个版本，在每一个单元格中存储了不同的值。
除了每个单元格可以保留若干个版本的数据这一点
所有的行按照行键字典序进行排序存储。
按照行键排序可以获得像RDBMS 的主键索引一样的特性，行键总是唯一的，并且只出现一次，否则你就是在更新同一行。HBase增加了辅助索引的支持。行键可以是任意的字节数组，但它并不一定是人直接可读的。
一行由若干列组成，若干列又构成一个列族（column famliy），这不仅有助于构建数据的语义边界或者局部边界，还有助于给它们设置某些特性，或者指示它们存储在内存中。一个列族的所有列存储在同一个底层的存储文件里，这个存储文件叫做HFile.
列族需要在表创建时就定义好，并且不能修改得了。列族名必须由可打印字符组成。
常见的引用列格式为 family:qualifier，qualifier 是任意的字节数组。与列族的数量有限制相反，列的数量没有限制：一个列族里可以有数百万个列。列值也没有类型和长度的限定。
图1-4 用可视化的方式展现了普通数据库与列式HBase在行设计上的不同，行和列没有像经典的电子表格模型那样排列，而是采用标签描述(tag metaphor)，也就是说，信息保存在一个特定的标签下。
所有列和行的信息都会通过列族在表中定义，
每一列的值或单元格的值都具有时间戳，默认由系统指定，也可以由用户显式设置。时间戳可以被使用，例如通过不同的时间戳来区分不同版本的值。一个单元格的不同版本的值按照降序排列在一起，访问的时候优先读取最新的值。这种优化的目的在于让新值比老值更容易读取。
用户可以指定每个值所能保存的最大版本数。此外，还支持谓词删除，
允许用户只保存过去一周内写入的值。这些值也只是未解释的字节数组，客户端需要知道怎样去处理这些值。
HBase是按照BigTable 模型实现的，是一个稀疏的、分布式的、持久化的、多维的映射，由行键、列键和时间戳索引。HBase数据存取模式如下：
（Table,RowKey,Family,Column,TimeStamp）-> Value
可以更像：
	SortedMap <
		RowKey,List<
			SortedMap<
				Column,List<
					Value,TimeStamp

或者：SortedMap<RowKey,List<SortedMap<Column,List<Value,TimeStamp>>>>>
第一个SortedMap 代表那个表，包含一个列族的List。列族中包含了另一个SortedMap存储列和相应的值。这些值在最后的list中，存储了值和该值被设置的时间戳。
行数据的存取操作时原子的，可以读写任意数目的列，目前还不支持跨行事务和跨表事务。原子存取也是促成系统架构具有强一致性的一个因素，因为并发的读写者可以对行的状态做出安全假设。

1.4.3 自动分区

HBase中扩展和负载均衡的基本单元称为Region，region 本质上是以行键排序的连续存储的区间。如果region 太大，系统就会把它们动态拆分，相反地，就把多个region 合并，以减少存储文件数量。
region等同于数据库分区中用的范围划分，它们可以被分配到若干台物理服务器上以均摊负载。
一张表初始只有一个region，用户开始向表中插入数据时，系统会检查这个region的大小，确保其不超过配置的最大值。如果超过了限制，系统会在中间键处将这个region 拆分成两个大致相等的子region。
每一个region只能由一台region服务器加载，每一台region服务器可以同时加载多个region。
region拆分和服务相当于其他系统提供的自动分区，当一个服务器出现故障后，该服务器上的Region可以快速恢复，并获得细粒度的负载均衡，因为当服务于某个region的服务器当前负载过大、发生错误或者停止使用导致不可用时，系统会将该region移到其他服务器上。
region拆分的操作也非常快----接近瞬间，因为拆分之后的region读取的仍是原存储文件，直到合并把存储文件异步地写成独立的文件。

1.4.4 存储API

API提供了建表、删表、增加列族和删除列族操作，同时还提供了修改表和列族元数据的功能，如压缩和设置块大小。此外，它还提供了客户端对给定的行键值进行增加、删除和查找操作的功能。
scan API提供了高效遍历某个范围的行的功能，同时可以限定返回哪些列或者返回的版本数。通过设置过滤器可以匹配返回的列，通过设置起始和终止的时间范围可以选择查询的版本。
在这些基本功能的基础上，还有一些更高级的特性。系统支持单行事务，基于这个特性，系统实现了对单个行键下存储的数据的原子读-修改-写序列。虽然还不支持跨行和跨表的事务，但客户端已经能够支持批量操作以获得更好的性能。
单元格的值可以当作计数器使用，并且能够支持原子更新。这个计数器能够在一个操作中完成读和修改，因此尽管是分布式系统架构，客户端依然可以利用此特性实现全局的、强一致性、连续的计数器。

还可以在服务器的地址空间中执行来自客户端的代码，支持这种功能的服务端框架叫做协处理器。这个代码能直接访问服务器本地的数据，可以用于实现轻量级批处理作业，或者使用表达式并基于各种操作来分析或汇总数据。
系统通过提供包装器集成了MapReduce 框架，该包装器能够将表转换成MapReduce 作业的输入源和输出目标。

与RDBMS 不同，HBase 系统没有提供查询数据的特定域语言，例如SQL。数据存取不是以声明的方式完成的，而是通过客户端API 以纯粹得到命令完成。HBase 的API 主要是java 代码，但也可以用其他语言实现。

1.4.5 实现

BigTable 允许客户端推断在底层存储中表示的数据的位置属性。

数据存储在存储文件（store file）中，称为HFile，HFile中存储的是经过排序的键值映射结构。文件内部由连续的块组成，块的索引信息存储在文件的尾部。当把HFile 打开并加载到内存中时，索引信息会有限加载到内存，每个块的默认大小是64KB，可以根据需要配置不同的块大小。存储文件提供了衣蛾设定起始和终止行键范围的API用于扫描特定值。

每一个HFile都有一个块索引，通过一个磁盘查找就可以实现查询。首先，在内存的块索引中进行二分查找，确定可能包含给定键的块，然后读取磁盘块找到实际要找的键。
存储文件通常保存在Hadoop 分布式文件系统中，HDFS 提供了一个可扩展的、持久、冗余的HBase 存储层。存储文件通过将更改写入到可配置数目的物理服务器中。以保证不丢失数据。
每次更新数据时，都会先将数据积累在提交日志中，在HBase 中这叫做预写日志，然后才会将这些数据写入内存的 memstore 中。一旦内存保存的写入数据的累积大小超过了一个给定的最大值，系统就会将这些数据移出内存作为HFile 文件刷写到磁盘。数据移出内存后，系统会丢弃对应的提交日志，只保留未持久化到磁盘中的提交日志，只保留未持久化到磁盘中的提交日志。在系统将数据移出 memstore 写入磁盘的过程中，可以不必阻塞系统的读写，通过滚动内存中的memstore 就能达到这个目的，即用空的新 memstore 获取更新数据，将满的旧 memstore 转换成一个文件。请注意， memstore 中的数据已经按照行键排序，持久化到磁盘中的HFile 也是按照这个顺序排列的，所以不必执行排序或其他特殊处理。

因为存储文件时不可被改变的，所以无法通过移除某个键/值对来简单地删除值。可行的解决方法是，做个删除标记，表明给定行已被删除。在检索过程中，这些删除标记掩盖了实际值，客户端读不到实际值。

读回的数据时两部分数据合并的结果，一部分是 memstore 中还没有写入磁盘的数据，另一部分是磁盘上的存储文件。数据检索时用不着WAL，只有服务器内存中的数据在服务器崩溃前没有写入到磁盘，而后进行恢复数据时才会用到WAL。

随着memstore 中的数据不断刷写到磁盘中，会产生越来越多的HFile 文件，HBase 内部由一个解决这个问题的管家机制，即用合并将多个文件合并成一个较大的文件。合并有两种类型：minor 合并和major压缩合并。minor合并将多个小文件重写为数量较少的大文件，减少存储文件的数量，这个过程实际上是多路归并的过程。因为HFile的每个文件都是经过归类的，所以合并速度很快，只受到磁盘IO性能影响。
major合并将一个region 中一个列族的若干个HFile 重写为一个新HFile，与minor 合并相比，还有更独特的功能：major 合并能扫描所有的键/值对，顺序重写全部的数据，重写数据的过程会略过做了删除标记的数据。断言删除此时生效，例如，对于那些超过版本号限制的数据以及生存时间到期的数据，在重写数据时就不再写入磁盘了。
这种架构来源于LSM树，唯一区别是，LSM树将多页块中的数据存储在磁盘中，其存储结构布局类似于B树。在HBase中，数据的更新与合并是轮流进行的，而在BigTable 中，更新是更粗粒度的操作，整个 memstore 会存储为一个新的存储文件，不会马上合并。可以把HBase的这种架构称为LSM映射，后台合并过程与LSM 树的结构合并过程相对应，只不过HBase 合并重写整个文件，而不会像LSM树一样只操作树结构的部分数据，LSM 树结构也正是因为这种操作得名。

HBase 中有3个主要组件：客户端库、一台主服务器、多台region 服务器。可以动态增加和移除region 服务器，以适应不断变化的负载。主服务器主要负责利用Apache Zookeeper 为 region 服务器分配 region，zookeeper 是一个可靠的、高可用的、持久化的分布式协调系统。

apache zookeeper
master 服务器负责跨 region 服务器的全局 region 负载均衡，将繁忙的服务器中的 region 移到负载较轻的服务器中。主服务器不是实际数据存储或者检索路径的组成部分，它仅提供了负载均衡和集群管理，不为region 服务器或者客户端提供任何的数据服务，因此是轻量级服务器。此外，主服务器该提供了元数据的管理操作，例如，建表和创建列族。
region 服务器负责为它们服务的 region 提供读写请求，也提供了拆分超过配置大小的region 接口。客户端则直接与 region 服务器通信，处理所有数据相关的操作。

1.4.6 小结

表的扫描与时间呈线性关系，行键的查找以及修改操作与时间呈对数关系---极端情况下是常数关系（使用了布隆过滤器）。HBase在设计上完全避免了显式的锁，提供了行原子性操作，这使得系统不会因读写性能而影响到系统扩展能力。

当前的行列存储结构允许表在实际存储时不存储NULL值，因此表可以看作是个无限、稀疏的表。表中每行数据只由一台服务器所服务，因此HBase 具有强一致性，使用多版本可以避免因并发解耦过程引起的编辑冲突，而且可以保留这一行的历史变化。

1.5.3 小节

HBase是一个分布式、持久化、强一致性的存储系统，具有近似最优的写性能和出色的读性能，它充分利用了磁盘空间，支持特定列族切换可选压缩算法。

hBase 继承自BigTable 模型，只考虑单一索引，类似于 RDBMS 中的主键，提供了服务器端钩子，可以实施灵活的辅助索引解决方案。此外，它还提供了过滤器功能，减少了网络传输数据量。

HBase 并未将说明性查询语言作为核心实现的一部分，对事务的支持也有限。但行原子性和‘读-修改-写‘操作在实践中弥补了这个缺陷，它们覆盖了大部分的使用场景并消除了在其他系统中经历过的死锁、等待问题。

HBase 在进行负载均衡和故障恢复时对客户端是透明的。在生产系统中，系统的可扩展性体现在系统自动伸缩的过程中。更改集群并不涉及重新全量负载均衡和数据重分区，但整个处理过程完全是自动化的。

3.1 客户端API 基础知识概述

HBase 的主要客户端接口是由 org.apache.hadoop.hbase.hbase.client包中的HTable 类提供的，通过这个类，用户可以完成向HBase存储和检索数据，以及删除无效数据，
所有修改数据的操作否是行级别的原子性，这会影响到这一行的数据所有的并发读写操作。换句话说，其他客户端或县城对同一行的读写操作都不会影响到改行数据的原子性: 要么等待系统允许写入该行修改。

通常，在正常负载和常规操作下，客户端读操作不会受到其他修改数据的客户端影响，因为他们之间的冲突可以忽略不计。但是，当许多客户端需要同时修改同一行数据时就会产生问题。所以，用户应当尽量使用批量处理更新来减少单独操作同一行数据的次数。

写操作中涉及的列的数目不会影响到该行数据的原子性，行原子性会同时保护到所有列。

最后，创建HTable 实例是有代价额。每个实例都需要扫描 .META 表，以检查该表是否存在、是否可用，此外还要执行一些其他操作，这些检查和操作导致实例调用非常耗时。因此，推荐用户只创建一次HTable 实例，而且是每个线程创建一个，然后在客户端应用的生存期内复用这个对象。

如果用户需要使用多个HTable 实例，应考虑使用 HTablePool 类，它为用户提供了一个复用多个实例的便捷方式。

只创建一次HTable 实例，一般在应用程序开始时创建
为执行的每一个线程（或者所使用的HTablePool）创建独立的HTable 实例。
所有的修改操作只保证行级别的原子性。

3.2 CRUD 操作

数据库的初始基本操作通常被称为CRUD，

3.2.1 put 方法
两类：一类操作用于单行，另一类操作用于多行。
1. 单行put
void put(Put put) throws IOException
这个方法以单个put 或存储在列表中的一组put对象作为输入参数，其中 Put 对象是由以下几个构造函数创建的：
Put(byte[] row);Put(byte[] row,RowLock rowLock);Put(byte[] row,long ts);
Put(byte[] row,long ts,RowLock rowLock)

HBase 友好地为用户提供了一个包含很多静态方法的辅助类，这个类可以把许多Java数据类型转换为byte[]数组。提供了方法部分清单：
Bytes类所提供的方法：
static byte[] toBytes(ByteBuffer bb)
static byte[] toBytes(String s)
static byte[] toBytes(boo

创建put实例后，就可以向该实例添加数据，添加数据的方法如下：
Put add(byte[] family,byte[] qualifier,byte[] value)
Put add(byte[] family,byte[] qualifier,long ts,byte[] value)
Put add(KeyValue kv) throws IOException

每一次调用add()都可以特定地添加一列数据，如果再加一个时间戳选项，就能形成一个数据单元格。注意，当不指定时间戳调用add方法时，Put实例会使用来自构造函数的可选时间戳参数，如果用户在构造Put实例时也没有指定时间戳，则时间戳将会由region 服务器设定

系统为一些高级用户提供KeyValue 实例的变种，这里高级用户是指知道怎样检索或创建这个内部类的用户。KeyValue 实例代表了一个唯一的数据单元格，类似于一个协调系统，该系统使用行键、列族、列限定符、时间戳指向一个单元格的值，像一个三维立方系统。

获取Put实例内部添加的Keyvalue实例需要调用与add()相反的方法 get()：
List<KeyValue> get(byte[] family,byte[] qualifier)
Map<byte[],List<KeyValue>> getFamilyMap()
以上两个方法可以查询用户之前添加的内容，同时将特定单元格的信息转换成KeyValue 实例。用户可以选择获取整个列族（column family）的全部数据单元，一个列族中的特定列或是全部数据。后面的getFamilyMap() 方法可以遍历Put 实例中每一个可用的KeyValue 实例，检查其中包含的详细信息。

每一个KeyValue 实例包含其完整地址（行键、列族、列限定符及时间戳）和实际数据。KeyValue 是HBase 在存储架构中最底层的类。

用户可以采用下方法检查是否存在特定的单元格，不需要遍历整个集合：
boolean has(

Put类提供的其他方法：
getRow()  返回创建Put实例时所指定的行键
getRowLock()	返回当前Put实例的行RowLock  实例。
getLockId()	返回使用rowlock参数传递给构造函数的可选的锁ID，当未被指定时返回-1L
setWriteToWAL()	允许关闭默认启用的服务器端预写日志功能
getWriteToWAL()	返回代表是否启用了WAL的值
getTimeStamp()	返回相应Put实例的时间戳，该值可在构造函数中由ts参数传入。当未被设定时返回Long.MAX_VALUE
heapSize()	计算当前Put实例所需的堆大小，既包含其中的数据，也包含内部数据结构所需的空间
isEmpty()	检测FamilyMap是否含有任何KeyValue实例
numFamilies()	查询FamilyMap的大小，即所有KeyValue 实例中列族的个数
size()	返回本次Put会添加的KeyValue 实例的数量

通过客户端代码访问配置文件
hbase-site.xml文件来获知如何访问集群
无论哪种方式，都需要在代码中使用一个HBaseConfiguration 类来处理配置的属性。可以使用该类提供的以下静态方法构建Configuration 实例：
static Configuration create()
static Configuration create(Configuration that)
第一个是默认create()。第二个方法允许使用一个已存在的配置，该配置会融合并覆盖HBase 默认配置
当调用任何一个静态create（）方法时，代码会尝试使用当前的Java classpath 来载入两个配置文件：hbase-default.xml 和 hbase-site.xml
当用户获得一个HBaseConfiguration 实例之后，其实已获得一个已经合并过的配置，其中包括默认值和在hbase-site.xml配置文件中重写的属性，以及一些用户提交的可选配置。在使用Htable实例之前，用户可以任意修改配置。例如，可以重写Zookeepeer的可用连接地址来定位到另一个集群：
Configuration config = HBaseConfiguration.create();
config.set("hbase.zookeeper.quorum","zk1.foo.com,zk2.foo.com");

换句话说，可以简单地忽略任何外部的客户端配置文件，而直接在代码中设置hbase.zookeeper.quorum属性。这要就创建了一个不需要额外配置的客户端。
同时应该共享配置实例，4.6节解释了这样做的原因。
现在又可以出踹死Shell命令来验证插入是否成功了：
>list
>scan 'testtable'

在创建Put实例时用到的另一个可选参数是ts，即时间戳，在HBase表中，时间戳使用户可以在HBase中将数据存储为一个特定版本。
System.currentTimeMillis()
如果需使用自定义的时间戳，就应该了解这些特性：
下面展示一个单元格插入并且获取多版本数据。
> create 'test','cf1'
> put 'test','row1','cf1','val1'
> put 'test','row1','cf1','val2'
> scan 'test'
> scan 'test','{versions => 3}

该示例在test 表中创建了一个名为 cf1的列族。两个 put 命令使用了相同的行键和列检，但他们的值不同：分别为 val1 和val2，然后使用scan 操作查看了这张表的所有内筒。
在HBase中，默认情况下，HBase会保留3个版本的数据，用户可以利用这种特性，稍稍修改scan 操作以便获取所有可获得的数据。示例最后一个命令列出了所有存储的数据版本。注意，即使所有输出的行键都是相同的，在shell 的输出中，所有的单元格都是以单独的一行输出的。

scan操作和get操作只会返回最后版本，因为HBase默认按照版本的降序存储，并且只返回一个版本。在调用中加入最大版本参数就可以获得多个版本的数据，如果将参数值设定为 Integer.MAX_VALUE,就可以获得所有版本。

正如最大版本的术语所表现出来的意思一样，对于一个特定额单元格，有可能只有少于最大版本数据版本，实例将version设为3，但是该单元格只存储了两个版本的数据，所以就列出了两个。

另一个获取多个版本数据的方法，使用时间范围参数。只需要设置开始时间和结束时间，就能获得所有满足时间范围的版本数据。

如果读者不指定该参数，当数据存储到底层文件系统时，RegionServer 会将当前行的时间戳隐式地设定为系统当前时间。

Put 类的构造数据还有一个名为 rowlock 的可选参数，它允许提交一个额外的行锁，若需要频繁地重复修改某些行，用户有必要创建一个RowLock 实例来防止其他客户端访问这些行。

2. KeyValue类
在代码中有时需要直接处理KeyValue 实例。你可能还记得之前讨论过，它们都含有一个特定单元格的数据以及坐标。坐标包括行键、列族名、列限定符以及时间戳。该类提供了特别多的构造器，允许以各种方式组合这些参数。下面展示了包括所有参数的构造器：
	KeyValue(byte[] row,int roffset,int rlength,byte[] family,int foffset,int flength,byte[] qualifier, int qoffset,int qlength,long timestamp, Type type,byte[] value,int voffset,int vlength)

建议：将KeyValue 类和它的比较器都设计为HBase 内部使用。只在客户端API 的几个地方出现，

数据和坐标都是以Java 的 byte[] 形式存储的，即以字节数组的形式存储，使用这种底层存储类型的目睹是：允许存储任意类型的数据，并且可以有效地只存储所需的字节，这保证了最少的内部数据结构开销。另一个原因是，每一个字节数组都有一个 offset 参数和一个 legth 参数，它们允许用户提交一个已存在的字节数据，并进行效率很高的字节级别操作。
坐标中任意一个成员都有一个getter方法，可以获得字节数组以及它们的参数 offset 和 length。不过也可以在最顶层访问它们，即直接读取底层字节缓冲区：
byte[] getBuffer()
int getOffset()
int getLength()

它们返回当前KeyValue 实例中字节数组完整信息。用户用到这些方法的场景很少，但在需要的时候，还是可以使用。
还有两个有意思的方法：
byte[] getRow(); byte[] getKey()
行（row）和键（key）有什么区别？行目前来说指的是行键，即Put构造器里的row参数。而在之前介绍的内容中，键是一个单元格的坐标，用的是原始的字节数组格式。在实践中，几乎用不到getKey()，但有可能会用到getRow().

KeyValue 类还提供一系列实现了 Comparator接口的内部类，可以在代码里使用它们来实现与HBase 内部一样的比较器。当需要使用API 获取KeyValue 实例时，并进一步排序或按顺序处理时，就要用到这些比较器。

表 3-2  KeyValue类提供的比较器的简要概述

KeyComparator		比较两个KeyValue 实例的字节数组格式的行键，即getKey()方法的返回值

KVComparator		是KeyComparator 的封装，基于两个给定的 KeyValue 实例，提供与 KeyComparator 一样的功能

RowComparator		比较两个KeyValue 实例的行键（getRow()的返回值）

MetaKeyComparator	比较两个以字节数组格式表示的.META. 条目的行键

MetaComparator		KVComparator类的一个特别版本，用于比较.META. 目录表中的条目，是MetaKeyComparator 的封装

RootKeyComparator	比较两个以字节数组格式表示的-root-条目的行键
RootComparator		KVComparator 类的一个特别版本，用于比较-Root-目录表中的条目，是RootKeyComparator 的封装
KeyValue 类将大部分的比较器按照静态实例提供给其他类使用。例如，由一个公有变量KEY_COMPARATOR，让用户可以访问KeyComparator 实例。COMPARATOR 变量指向使用更频繁的KVComparator 实例。所以可以不用创建自己的实例。而是使用提供额实例，可以按照以下方法创建一个KeyValue 实例的集合，这个集合可以按照 HBase 内部使用的顺序来排序：

TreeSet<KeyValue> set = new TreeSet<KeyValue>(KeyValue.COMPARATOR)

KeyValue 实例还有一个变量，代表该实例的唯一坐标：类型。
表 3-3 KeyValue 实例所有可能的类型值
Put		KeyValue 实例代表一个普通的Put操作
Delete 	KeyValue实例代表一个Delete操作，墓碑标记
DeleteColumn	与Delete相同，但是会删除一整列
DeleteFamily	

可以通过使用另一个方法来查看一个KeyValue实例的类型，例如：
String toString()
按照以下格式打印当前KeyValue 实例的元信息：
<row-key>/<family>:<qualifier>/<version>/<type>/<value-length>

这个方法会用在本书的一些示例代码中，用于检查山水是否被标记或者被恢复，同时也可以查看元信息

该类有很多更便捷的方法：允许对存储数据的其中一部分进行比较，检查实例的类型是什么，获得它已经计算好的堆大小，克隆或者复制该类等。有一些静态方法可以创建一些特殊的KeyValue 实例，用以在HBase 内更底层地比较或者操作数据。可以参考

3. 客户端的写缓冲区

每一个Put操作实际的是一个RPC 操作，它将客户端数据传送到服务器然后返回，这只适合小数据量的操作，如果

减少独立RPC 调用的关键是限制往返时间，往返时间就是客户端发送一个请求到服务器，然后服务器通过网络进行响应的时间。这个时间不包含数据实际传输时间，它其实是通过线路传送网络包的开销，一般情况下，在LAN网络中大约要花1毫秒的时间，这意味着在1秒内只能完成1000次 RPC 往返响应。

另一个重要的因素就是消息大小。如果通过网络发送的请求内容较大，那么需要请求返回的次数相应较少，因为是缠绵主要话费在数据传递上，不过如果传送的数据量小，比如一个计数器递增操作，那么用户把多次修改得数据批量提交给服务器并减少请求次数，性能会有相应提升。

HBase 的API 配备了一个客户端的写缓冲区，缓冲区负责收集put操作，然后调用RPC操作一次性将put送往服务器。全局交换机控制着该缓冲区是否在使用，以下：
void setAutoFlush(boolean autoFlush)
boolean isAutoFlush()

默认情况下，客户端缓冲区是禁用的。可以处通过将自动刷写设置为false 来激活缓冲区，调用如下：
table.setAutoFlush(false)

启用客户端缓冲机制后，用户可以通过isAutoFlush() 方法检查标识的状态。当用户初始化创建一个HTable 实例时，这个方法将返回true，如果有用户修改过缓冲机制，它会返回用户当前所设定的状态。

激活客户端缓冲区后，将数据存储到HBase 中，此时操作不会产生RPC 调用，因为存储的Put 实例保存在客户端进程的内存中。当需要强制把数据写到服务端时，可以调用另一个API 函数：
void flushCommits() throws IOException

flushCommits() 方法将所有修改传送到远程服务器。被缓冲的Put 实例可以跨多行。客户端能够批量处理这些更新，并把它们传送到对应的region 服务器。和调用单行 put() 方法一样，用户不需要担心数据分配到哪里，因为对于用户来说，HBase客户端对这个方法的处理时透明的。

用户可以强制刷写缓冲区，不过这通常不必要，因为API 会追踪统计每个用户添加的实例的堆大小，从而计算出缓冲的数据量。除了追踪所有数据开销，还会追踪必要的内部数据结构，一旦超出缓冲指定大小限制，客户端就会隐式地调用刷写命令。用户可以通过以下调用来配置客户端写缓冲区大小：
long getWriteBufferSize()
void setWriteBufferSize(long writeBufferSize) throws IOException

默认的大小是2MB ，这个大小比较适中，一般用户插入HBase 中的数据都相当小，即每次插入的数据都远小于缓冲区大小。如果需要存储较大数据，可能就需要考虑增大这个数值，从而允许客户端更高效地将一定数量的数据组成一组，通过一个RPC 请求来执行。

给每一个用户创建HTable 实例都设定缓冲区大小麻烦，可以在 hbase-site.xml 配置文件中添加一个较大的预设值。例如：
<property>
	<name>hbase.client.write.buffer</name>
	<value>20971520</value>
</property>

缓冲区仅在以下两种情况下刷写：
显式刷写：
用户调用 flushCommits()方法，把数据发送到服务器做永久存储
隐式刷写：
隐式刷写会在用户调用put（）或setWriteBufferSize() 方法时触发。这两个方法都会将目前占用额缓冲区大小与用户配置的大小做比较，如果超出限制则会调用flushCommit() 方法。如果缓冲区被禁用，可以设置setAutoFlush(true)， 这样用户每次调用 put() 方法时都会触发刷写。此外调用HTable 类的close() 方法时也会无条件地隐式触发刷写。
get()操作用于从服务器读取数据，

用户可以使用如下方法访问客户端写缓冲区的内容：ArrayList<Put> getWriteBuffer()，这个方法可以获取 table.put(put)添加到缓冲区中Put 实例列表。

由于该列表，使得HTable 类被多个线程操作时不安全，直接操作那个列表
由客户端缓冲区是一个简单的保存在客户端进程内存里的列表，用户需要注意不能再运行时终止程序。如果发生这种情况，那些尚未刷写的数据就会丢失，服务器将无法收到数据，因此这些数据没有任何副本可以用来做数据恢复。
一个更大额缓冲区需要客户端和服务器端消耗更多的内存，因为服务器也需要先将数据写入到服务器的写缓冲区中，然后再处理它，另一方面，一个大额缓冲区减少了PRC 请求的次数。估算服务器端内存的占用可使用 hbae.client.write.buffer * hbase.regionserver.handler.count * region服务器的数量

4. Put列表
客户端的API 可以插入单个Put 实例，同时也有批量处理操作的高级特性。其调用形式如下：
 void put(List<Put> puts) throws IOException
 
用户需要建立一个Put 实例的列表。

由于用户提交的修改行数据的列表可能涉及多行，所以有可能会由部分修改失败。造成修改失败的原因很多，例如，一个远程的regioin 服务器出现问题，导致客户端的重试次数超过配置的最大值，因此不得不放弃当前操作，如果远程服务器的put调用出现问题，错误会通过随后的一个IOException 异常反馈给客户端。

服务器遍历所有操作并设法执行，失败的返回，然后客户端会使用RetriesExhaustedWithDetailsException 报告远程错误，这样用户可以查询有多少个操作失败、出错的原因以及重试的次数。注意：对于错误列族，服务器端的重试次数会自动设为1，因为

这些在服务器失败的Put 实例会被保存在本地写缓冲区中，下一次缓冲区刷写的时候会重试。用户可以通过HTable 的 getWriteBuffer() 方法访问它们，并对它们做一些处理，例如，消除操作

有一些检查实在客户端完成，例如，确认Put 实例的内容是否为空或是否制定了列，在这种情况下，客户端会抛出异常，同时将出错的Put 留在客户端缓冲区中不做处理。

调用基于列表的put()时，客户端会先把所有的Put 实例插入到本地写缓冲区中，然后隐式地调用 flushCache() 。在插入每个Put 实例时，客户端API 都会执行之前提到过的检查。如果检查失败，例如，5个Put 中的第3个失败了，则前两个会被添加到缓冲区中，最后两个则不会，同时也不会触发刷写命令。

用户可以捕获异常并手动刷写写缓冲区来执行已经添加的操作，

当使用基于列表的put 调用时，用户需要特别注意：用户无法控制服务器端执行 put 的顺序，这意味着服务器被调用的顺序也不受用户控制。如果要保证写入的顺序，需小心地使用这个操作，最坏的情况是，要减少每一批处理的操作数，并显示地刷写客户端写缓冲区，强制把操作发送到远程服务器。

5. 原子性操作 compare-and-set

有一种特别的 put 调用，能保证自身操作的原子性：检查写。该方法的签名如下：
 boolean checkAndPut(byte[] row,byte[] family,byte[] qualifier,
 byte[] value,Put put) throws IOException
 
 有了这种带有检查功能的方法，就能保证服务器端put 操作的原子性。如果检查成功通过，就执行put操作，否则就彻底放弃修改操作。这种操作方法可以用于需要检查现有相关值，并决定是否修改数据的操作。
 
 这种有原子性保证的操作经常被用于账户结余、状态转换或数据处理等场景，这些应用场景的共同点是，在读取数据的同时需要处理数据。一旦你想把一个处理好的结果写回HBase，并保证没有其他客户端已经做了同样的事情，你就可以使用这个有原子性保证的操作，先比较原值，再做修改。
 
 有一种特别的检查通过checkAndPut() 调用来完成，即只有在另外一个值不存在的情况下，才执行这个修改。要执行这种操作只需要将参数value 设置为null 即可，只要指定列不存在，就可以成功执行修改操作。
 
 这个方法返回一个布尔类型的值，表示put 操作成功执行还是失败，对应的值分别是 true 和false。
 
 HBase 提供的compare-and-set 操作，只能检查和修改同一行数据。与其他的许多操作一样，这个操作只提供同一行数据的原子性保证。检查和修改分别针对不同行数据时会抛出异常
 
 3.2.2 get方法
 
 从客户端API 中获取已存储数据的方法。HTable 类中提供了 get() 方法，同时还有与之对应的 Get 类。get方法分为两类：一类是一次获取一行数据；另一类是一次获取多行数据。
 1. 单行get
 这种方法可以从HBase 表中获取一个特定的值：
	Result get(Get get) throws IOException
与put()方法有对应的Put 类相似，get()方法也有对应的 Get()类，此外还有一个相似之处，那就是在使用下面的方法构造 Get实例时，也需要设置行键：
Get(byte[] row)
Get(byte[] row,RowLock rowLock)

虽然一次get()操作只能取一行数据，但不会限制一行当中取多少列或者多少单元格。

这两个Get 实例都通过 row 参数指定了要获取的行，其中第二个Get 实例增加了一个可选的 rowLock 参数，允许用户设置行锁。与 put 操作一样，用户有许多方法可用，可以通过多种标准筛选目标数据，也可以指定精确的坐标获取某个单元格的数据：
	Get addFamily(byte[] family)
	Get addColumn(byte[] family,byte[] qualifier)
	Get setTimeRange(long minStamp,long maxStamp) throws IOException
	Get setTimeStamp(long timestamp)
	Get setMaxVersions()
	Get setMaxVersions(int maxVersions) throws IOException

addFamily() 方法限制get 请求只能取得一个指定的列族，要取得多个列族时需要多次调用。addColumn()的使用方式与之类似，用户通过它可以指定get 取得哪一列的数据，从而进一步缩小地址空间。有一些方法允许用户设定要获取的数据的时间戳，或通过设定一个时间段来取得时间戳属于该时间段内的数据。

最后，如果用户满意设定时间戳的话，也有方法允许用户设定要获取的数据的版本数目。默认情况下，版本数为1，即get() 请求返回最新匹配版本。不带参数的setMaxVersions()方法会把要取出的最大版本数设为 Integer.MAX_VALUE，这是用户在列族描述符中可配置的最大版本数，此时

表3-4	Get类提供的其他方法概览

getRow()	返回创建Get 实例时指定的行键
getRowLock()	返回当前Get实例的Rowlock实例
getLockId()	返回创建时指定rowLock 的锁ID,如果没有指定则返回-1L
getTimeRange() 返回指定的Get 实例的时间戳范围，注意，get类中已经没有getTimeStamp()方法了，因为API会在内部将setTimeStamp()赋的值转换成TimeRange 实例，设定给定时间戳的最大值和最小值
setFilter()/getFilter() 用户可以使用一个特定的过滤器实例，通过多种规则和条件来筛选列和单元格，使用这些方法，用户可以设定或岔开get实例的过滤器成员
setCacheBlocks()/getCacheBlocks() 每个HBase 的region 服务器都有一个块缓存来有效地保存最近存取过的数据，并以此来加速之后的相邻信息的读取。不过在某些情况下，例如完全随机读取时，最好能避免这种机制带来的扰动，这些方法能控制当次读取的块缓存机制是否启效
numFamilies()	快捷地获取列族FamilyMap 大小的方法，包括有addFamily()方法和addColumn()方法添加的列族

hasFamilies()	检查列族或列是否存在于当前的Get 实例中

familySet()/getFamilyMap()	这些方法能让用户直接访问addFamily() 和 addColumn() 添加的列族和列，FamilyMap 列族中键是列族的名称，键对应的值是指定列族的限定符列表。familySet()方法返回一个所有已存储列族的Set,即一个只包含列族名的集合。

表3-4 中所列的 getter 函数只能得到所属的 Get 实例中用户预先设定好的筛选条件。它们的应用场景很少，而且只能在类似如下的场景中使用，例如,用户的一个私有方法 中有一个Get 实例，需要在其他地方检查Get 实例中各筛选条件是否正确。

HBase 为用户提供了Bytes 这个工具类，该类有许多可以把Java 常用数据类型转化为 byte[] 数组的静态方法，同时，它也可以做一些反向转化的工作：例如当用户从HBase 中取得一行数据时，可使用 Bytes 对应的方法把byte[] 内容转化为Java 数据类型。下面是相关方法列表：
static String toString(byte[] b)
static boolean toBoolean(byte[] b)
static long toLong(byte[] bytes)
static float toFloat(byte[] bytes)
static int toInt(byte[] bytes)

2. Result 类

当用户使用get() 方法获取数据时，HBase 返回的结果包含所有匹配的单元格数据，这些数据将被封装在一个Result 实例中返回给用户，用它提供的方法，可以从服务器端获取匹配指定行的特定返回值，这些值包括列族、列限定符和时间戳等。
如果用户之前要求服务器端返回一个列族下的所有列，现在就可以从返回值中取得这个列族下所需的任意列。换句话说，用户使用get() 方法时需要提供一些具体的信息，以便数据取回之后客户端可以筛选出对应的数据，Result 类提供的方法如下：
byte[] getValue(byte[] family,byte[] qualifier)
byte[] value()
byte getRow()
int size()
boolean isEmpty()
KeyValue[] raw()
List<KeyValue> list()

getValue() 方法允许用户取得一个HBase 中存储的特定单元格的值。因为该方法不能指定时间戳，所有用户只能获得数据最新版本。value()方法的使用：它会返回第一个列对应的最新单元格的值。因为列在服务器端是按字典序存储的，所以会返回名称（包括列族和限定符）排在首位的那一列的值。
getRow():它返回创建Get()类当前实例时使用的行键。size() 方法返回服务器端返回值中键值对（KeyValue实例）的数目。用户可以使用size（）方法或isEmpty() 方法查看键值对的数目是否大于0，这样可以检查服务器端是否找到了与查询。

raw() 方法返回原始的底层KeyValue 数据结构，具体来说，是基于当前的Result 实例返回KeyValue 实例的数组。list()调用则把raw() 中返回的数组转化为一个List 实例，并返回给用户，创建的List 实例由原始返回结果中 KeyValue 数组成员组成，用户可以方便的迭代存储数据。

另外还有一些面向列的存取函数：
List<KeyValue> getColumn(byte[] family,byte[] qualifier)
KeyValue getColumnLatest(byte[] family,byte[] qualifier)
boolean containsColumn(byte[] family,byte[] qualifer)

以下是第三类取值函数，以映射形式返回结果：
NavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>>

最常用的方法时getMap()，它把所有的get()请求返回的内容都装入一个 Java 的Map 类实例中，这样用户可以使用该方法遍历所有结果，getNoversionmap() 与 getMap() 形式上相似，不过它只返回每个列的最新版本，getFamilyMap() 允许用户指定一个特定的列族，返回这次结果中这个列族下的所有版本。

转储内容

所有的Java 对象都有toString() 方法，这个方法通常会被重载，用于将实例数据转化为文本表示，这样做一般不是为了序列化对象，而是为了放标调试程序。
同样Result 类也有 toString() 方法的实现，它把实例的内容转储为一个可读的字符串，
这个方法只是简单地打印实例所包含的 KeyValue 实例，也就是逐个调用 KeyValue.toString()
如果Result的实例为空，则返回结果：keyvalues = 

3. Get 列表

实际上，请求有可能被发往多个不同服务器，但这部分逻辑已经
API 提供的方法签名如下：
Result[] get(List<Get> gets) throws IOException
用户需要创建一个列表，并把之前准备好的Get 实例添加到其中，然后将这个列表传给get()，会返回一个与列表大小相等的 Result 数组。

4. 获取数据的相关方法

boolean exists(Get get) throws IOException
可以和使用HTable 的 get()方法一样，先创建一个 Get 类的实例，exists() 方法通过RPC 验证请求的数据是否存在，但不会从远程服务器返回请求的数据，只返回一个布尔值表示这个结果。

exists() 方法会引发 region 服务器端查询数据的操作，包括加载文件块来检查某行或某列是否存在，用户通过这种方法只能避免网络数据传输的开销，不过在需要检查或频繁检查一个比较大的列时，这种方法还是
Result getRowOrBefore(byte[] row,byte[] family) throws IOException
用户需要指定要查找的行键和列族。HBase是一个列式存储的数据库，不存在没有列的行数据。设定列族之后，服务器端会检查要查找的哪一行里是否有任何属于指定列族的列值

可以从getRowOrBefore() 返回的Result 实例中得到要查找的行键。这个行键要么与用户设定的行一致，要么刚好是设定行键之前的一行，如果没有匹配的结果，本方法返回null。如果没有匹配的结果，本方法返回null。


2. Delete 列表

基于列表的delete()调用与基于列表的put()相似，需要创建一个包含Delete实例的列表，对其进行配置，并调用下面方法：
	void delete(List<Delete> deletes) throws IOException

最后介绍的是基于列表的delte(0操作异常处处，下面对传入deletes参数做修改，使得在调用返回时，还有一个错误的Delete 实例，换句话说，如果所有操作成功，这个列表为空，但是，如果最后还有一个实例的话，远程服务器会报告这个错误，这个调用也要抛出异常。用户需要用

3. 原子性操作 compare-and-delete

前文已经在 “原子性操作compare-and-set” 一节介绍过，
boolean checkAndDelete(byte[] row,byte[] family,byte[] qualifier,
	byte[] value,Delete delete) throws  IOException
	
用户必须指定行键、列族、列限定符和值来执行删除操作之前的检查。如果检查失败，则不执行删除操作，调用返回 false ，如果检查成功，则执行删除操作，调用返回true。
如果原子性由客户端保证，那么就要对整行数据加排他锁，如果在已加锁情况下客户端崩溃，那么服务器端必须通过超时机制对数据解锁。这样也需要额外的RPC 请求，这比一次服务器端的操作慢

3.3 批量处理操作

事实上，许多基于列表的操作，都是基于batch() 方法实现的，它们都是一些为了方便用户使用而保留的方法。

休眠客户端API 方法提供了批量处理操作，用户可能注意到这里引入了一个新的名为Row 的类，它是 Put、Get 和 Delete 的祖先或者父类

void batch(List<Row> actions,Object[] results) throws IOException,InterruptedExcpetion

Object[] batch(List<Row> actions) throws IOException,InterruptedExcpetion
使用同样的父类允许在列表中实现多态，即放入以上3种不同

不可以将针对同一行的Put 和 Delete 操作放在同一个批处理请求中，为了保证最好的性能，这些操作处理顺序可能不同，但是这样会产生不可预料的结果，由于资源竞争，某些情况下，用户会看到波动结果。

batch() 调用可能的返回结果
null	操作与远程服务器的通信失败
EmptyResult Put 和 Delete操作成功后的返回结果
Result	Get 操作成功的返回结果，如没有匹配额定行或列，会返回空的 Result
Throwable	当服务器返回一个异常时，这个异常会按原样返回客户端，用户可以使用这个异常检查

当用户使用batch() 功能时，Put 实例不会被客户端写入缓冲区缓冲。batch() 请求是同步的，会把操作直接发送到服务器端，这个过程没有什么延迟或其他中间操作。这与put() 调用不同。

有两种不同的批处理操作，一个需要用户输入包含返回结果的Object 数组，另一个由函数帮助用户创建这个数组。关键的不同在于：
void batch(List<Row> actions,Object[] results) throws IOException,InterruptedExcpetion
这个方法可以访问部分结果，而下面这个方法不行：
Object[] batch(List<Row> actions) throws IOException,InterruptedExcpetion
后面这个方法如果抛出异常的话，不会有任何返回结果，因为新结果数组返回之前，控制流就中断了。
而之前的方法会先向用户提供的数组中填充数据，然后再抛出异常。
两种方法共同点：
	get、put 和 delete 都支持，如果执行时出现问题，客户端将抛出异常并报告，它们都不使用客户端写缓冲区。
void batch(actions,results) 能够访问成功操作的结果，同时也可以获取远程失败时的异常
Object[] batch(actions) 只返回客户端异常，不能执行城西执行中的不能结果。

在检查结果之前，所有的批处理操作都被执行：即使用户收到一个操作的异常，其他操作也都执行了。不过，在最坏的情况下，可能所有操作都会返回异常。
另外，批处理可以感知暂时性错误，例如 NotServingRegiionException（表明一个Region已经被移动）会多次重试这个操作，用户可以通过调整hbase.client.retries.number 配置项（默认是10）来增加或减少重试次数。

3.4 行锁

像put()、delete()、checkAndPut() 这样的修改操作时独立执行的，这意味着在一个串行方式的执行中，对每一行必须保证行级别的操作时原子的，region 服务器提供了一个行锁的特性，这个特性保证了只有一个客户端获取一行数据相应的锁，同时对该行进行修改。在实践中，大部分客户端应用程序都没有提供显式的锁，而是使用这个机制来保障每个操作的独立性。

用户应尽可能避免使用行锁，就像在RDBMS 中，死锁。
锁超时之前，两个被阻塞的客户端会占用一个服务器端的处理线程
当使用put() 访问服务器时，Put 实例可以通过以下构造函数
Put(byte[] row)

这个构造函数没有 RowLock实例参数，所以服务器会在调用期间创建一个锁。实际上，
客户端加锁：
RowLock lockRow(byte[] row) throws IOException
void unlockRow(RowLock rl) throws IOException

第一个调用lockRow() 需要一个行键作为参数，返回一个 RowLock 实例，这个实例可以供后续 put 或 Delete 构造函数使用，一旦不在需要锁，unlockRow()释放。

每一个排他锁（unique lock），无论是服务器提供，还是通过客户端API 传入，都能保护着一行不被其他锁锁定。锁必须针对整个行，并且指定其行键，一旦它获得锁定权就能防止其他的并发修改。

当一个锁被服务器端或客户端显式获取之后，其他所有想要对这行数据加锁的客户端将会等待，直到当前所被释放，

一个显示锁是如何阻塞另一个使用隐式锁额线程的，主线程休眠了5秒，一醒来就调用了两次put()，分别将同一列设置为两个不同的值。

主线程的锁已释放，阻塞线程的run()方法就继续执行并调用了第三个put。观察put 操作在服务器端的执行情况，KeyValue 实例的时间戳显示第三个 put 拥有最小时间戳，虽然这个 put 表面上是最后执行的。这是因为线程中的 put() 调用时再两个主线程中的 put() 之前执行的，这之后主线程休眠了5秒。

3.5 扫描

这种技术类似于数据库系统中的游标，并利用到了HBase 提供的底层顺序存储的数据结构

3.5.1 介绍

扫描跟get() 方法类似，同样，和其他函数类似，这里也提供了Scan 类。但是由于扫描操作工作方式类似于迭代器，所以用户无需调用scan() 方法创建实例，只需调用HTable的 getScanner() 方法，此方法在返回真正的扫描器实例的同时，用户也可以使用它迭代获取数据。可用方法如下：
 ResultScanner getScanner(Scan scan) throws IOException
 ResultScanner getScanner(byte[] family) throws IOException
 ResultScanner getScanner(byte[] family,byte[] qualifier) throws IOException
 后两个隐式帮用户创建了一个 scan 实例，逻辑中最后调用 getScanner(Scannscan) 方法
 
 Scan 类有以下构造器：
	Scan()
	Scan(byte[] startRow,Filter filter)
	Scan(byte[] startRow)
	Scan(byte[] startRow,byte[] stopRow)
	
这与 Get 类的不通电：用户可以选择性提供 startRow 参数，来定义扫描读取 HBase 表的起始行键，即行键不是必须指定的。同时可选 stopRow 参数用来限定读取到何处停止。
起始行包括在内，中止行不包括在内。

用户提供的参数不必精确匹配这两行，扫描会匹配相等或大于给定的起始行的行键。如果没有显示指定起始行，会从表的起始位置开始获取数据。

当遇到与设置的终止行相同或大于终止行的行键时，扫描也会停止。如果没有指定终止行键，会扫描到表尾。

另一个可选参数叫过滤器，可直接指向Filter 实例，尽管 Scan 实例通常由空白构造器构造，但其所有可选参数都有对应的 getter 方法和 setter 方法

创建Scan 实例后，用户可能还要给它增加更多限制条件，用户仍然可以使用空白参数的扫描，它可以读取整个表格，包括所有列族以及它们所有列。可以用多种方法限制要读取的数据：
 Scan addFamily(byte[] family)
 Scan addColumn(byte[] family,byte[] qualifier)
 
 这里有很多类似Get 功能： 可以使用 addFamily() 方法限制返回数据的列族，或通过 addColumn() 方法限制返回的列。
 
 Scan setTimeRange(long minStamp,long maxStamp) throws IOException
 Scan setTimeRange(long timeStamp)
 Scan setMaxVersions()
 Scan setMaxVersions(int maxVersions)
 
 用户可以同
 
 Scan setStartRow(byte[] startRow)
 Scan setStopRow(byte[] stopRow)
 Scan setFilter(Filter filter)
 boolean hasFilter()
还可以使用 setStartRow()、setStopRow()\setFilter()，进一步限定解放者的数据

Scan 类其他方法概览
getStartRow()/getStopRow()		查询当前设定的值
getTimeRange()			检索Get 实例指定的时间戳范围或相关时间戳，注意，当需要指定单个时间戳时，API 会在内部通过setTimeStamp() 将 Timerange 实例起止时间戳设为传入值，所以 Get 类中此时已经没有 getTimeStamp() 方法

getCacheBlocks()/setCacheBlocks()	每个HBase的region 服务器都有一个块缓存，可以有效地保存最近访问数据，并以此来加速之后相邻信息的读取，不过在某些情况下，例如全表扫描，最好能避免这种机制带来的扰动，这个方法能控制本次赌气的块缓存机制是否启效。

numFamilies()	快捷地获取FamilyMap 大小的方法，包括有 addFamily() 和 addColumn() 方法添加的列族和列。

一旦设置好了 Scan 实例，就可以调用 HTable 的 getScanner() 方法，获得用于检索数据的 ResultScanner 实例

3.5.2 ResultScanner 类

扫描操作不会通过一次 RPC 请求返回所有匹配的行，而是以行为单位进行返回，很明显，行的数目很大，可能有上千条甚至更多，同时在一次请求中发送大量数据，会占用大量的系统资源并消耗很长时间。

ResultScanner 把扫描操作转化为类似的 get 操作，它将每一行数据封装成一个 Result 实例，将所有 Resultuili fhfru iyige 地带器中， ：
Result next() throws IOException
Result[] newxt(itnt nbRows) throws IOException
void close()

有两种类型的 next() 调用供用户选择，调用 close() 方法会释放所有由扫描控制的资源

扫描器租约
要确保今早释放扫描器实例，一个打开的扫描器会占用不少服务端资源，累积多

<property>
	<name>hbase.regionservier.lease.period</name>
	<value>120000</value>
</property>
这个值同时适用于锁租约和扫描器租约

在休眠一定时间后，尝试迭代获取扫描器提供的行，由于租约超时，这个操作触发服务器端超时异常，同时返回的异常信息还包括当前配置的超时时间。

修改超时时间使用conf.setLong(HConstants.HBASE_REGIONSERVER_LEASE_PERIOD_KEY,120000)是不起作用的，是客户端修改，而非服务器端修改，如要服务器端修改则要在服务器端修改hbase-site.xml 文件，完了重启服务器配置

ScannerTimeOutException 异常时如花包装在UnknowScannerException 异常的外面抛出，异常信息表明扫描器的 next() 方法使用扫描器 ID 在服务器端查找已经建立的扫描器，但由于这个扫描器 ID 的租约超时，已经被删除，换句话，客户端缓存的扫描器ID 在 region 服务器已经找不到，

到目前为止，使用客户端的扫描器缓存来从远程region 服务器向客户端整批传输数据，数据量非常大的行，可能拆过客户让进程的内存容量。HBase 和它的客户端 API 对这个问题有：批量，使用以下方法控制批量获取操作：
void setBatch(int batch)
int getBatch*()

缓存面向行操作，而批量是面向列的操作。批量可以让用户选择每一次 ResultScanner 实例的 next() 操作要取回多少列。例如，在扫描中设置 setBatch(5)，则一次 next() 返回的 Result 实例会包括5列。

如果一行包括的列数超过了批量中设置的值，则可以将这一行分片，每次 next 操作返回一片。

当一行的列数不能被批量中设置的值整除时，最后一次返回的 Result 实例会包含比较少的列，如果一行有17列，用户把 batch 值设为5，则一共会返回4个 Result 实例，这4个实例中包括的列数应当分别为5、5、5、2.

组合使用扫描器缓存和批量大小，可以让用户方便地控制扫描一个范围内的行键时所需要的 RPC 调用次数。

代码打印出这两个参数的值、服务器返回的 Result 实例数目以及获取数据过程所发起的 RPC 请求数据，结果如下：

Caching: 1, Batch: 1, Results: 200, RPCs: 201
Caching: 200, Batch: 1, Results: 200, RPCs: 2
Caching: 2000, Batch: 100, Results: 10, RPCs: 1

用户可以修改调整着两个参数来查看他们对输出结果的影响。表 3-9 展示了一些组合。建立了一张有两个列族的表，添加了10 行数据。每个行的每个列族下有10 列。这意味着整个表一共有200 列(或单元格，因为每个列只有一个版本)，其中每行有20列。

小的批量值使服务器端把3 个列装入一个 Result 实例，同时扫描器缓存为6，使每次 RPC 请求传输 6行，即6个被批量封装的 Result 实例。如果没有指定批量大小，但指定了扫描器缓存，那么一个调用结果就能包含所有的行，因为每一行都包含在一个 Result 实例中。只有当用户使用批量模式之后，行内扫描功能才会启用。

最初，用户可能不必为扫描器缓存和批量模式的使用操心，但当用户想尽量提高和利用系统性能时，可能就需要为这两个参数选择一个合适的组合了。

3.6 各种特性

3.6.1 HTable 的实用方法
客户端 API 是由HTable 类的实例提供的，用户可以用它来操作 HBase 表。除之前提到过的一些主要特性外，还有以下一些值得注意的方法：
	bte[] getTableName()
	这是一个获取表名称的快捷方法
	Configuration getConfiguration()
	这个方法
	void clearRegionCache()
	HRegionLocation getRegionLocation(row)
	Map<HRegionInfo,HServerAddress> getRegionsInfo()
	这些方法可以帮助用户获取某一行数据的具体位置信息，或者说这行数据所在的region 信息，以及所有 region 的分布信息。用户也可以在必要的时候清空缓存的 region 位置信息。这些方法可以让高级用户了解并使用这些信息，例如，控制集群流量或者调整数据的位置。

	void prewarmRegionCache(Map<HRegionInfo,HServerAddress> regionMap)
	static void setRegionCachePrefetch(table,enable)
	static boolean getRegionCachePrefetch(table)
	这是一组高级方法，可以先预取 region 位置信息来避免耗时较多操作（查找每行数据所属 region 地址直到本地 region 地址缓存相对稳定）用户可以先获取一个 region 信息表来预热一下 region 缓存 （例如，用户可以使用 getRegionsInfo() 访问这个 region 的信息表，然后再处理它），也可以把整张表的预取功能打开

3.6.2 Bytes 类

	例如，将 String 或 long 转化为 HBase 原生支持得原始字节数组，
	大部分方法都有3 种形式：
	static long toLong(byte[] bytes)
	static long toLong(byte[] bytes,int offset)
	static long toLong(byte[] bytes,int offset,int length)

用户可以输入一个字节数组，或者一个字节数组再加一个偏移值，或者一个字节数组、一个偏移值和一个长度值。具体使用哪一种方法取决于最初这个字节数组是增么生成的，如果这个字节数组之前是由 toBytes() 方法生成的，用户就可以安全地使用第一种形式的方法将其转化回来，只需送入这个字节数组而不需要其他额外信息，这个数组的内容都是转换过来的值。

不过，API 和 HBase 内部都把数据存储为一个较大的数组，：
static int putLong(byte[] bytes,int offset,long val)
这个方法允许用户把一个 long 值写入一个字节数组的特定偏移位置。用户可以使用后面的两种 toLong() 方法存取这种较大的字节数据的数据。

Bytes 类支持一下原生 Java 类型到字节数据互转： String、 boolean、short、int、long、double 和 float。

表3-10 Bytes 类提供的其他方法概述

toStringBinary()		该方法可以安全地把不能打印的信息转换为人工可读的十六进制，如果用户不清楚字节数组中内筒，可以使用该方法

compareTo()/equals()	这个方法让用户可以对两个 byte[]()进行比较，前者返回一个比较结果

add()/head()/tail()		用这些方法可以把两个字节数组连接在一起形成一个新的数组，或者可以取到字节数组头或尾的一部分

binarySearch()			在用户给定的字节数组中二分查找一个目标值，该方法可以在字节数组上对用户要查找的值和键进行操作

incrementBytes()		一个 long 类型数据转化成的字节数组与 long 的数据相加并返回字节数组，用户可以使用负数参数进行减法

第 4 章： 客户端 API: 高级特性

4.1 过滤器
HBase 过滤器提供了强大的特性来帮助用户提高其处理表中数据的效率。用户不仅可以使用 HBase 中预定义好的过滤器，而且可以实现自定义的过滤器。

4.1.1 过滤器简洁

HBase 中两种主要的数据读取函数是 get() 和 scan(), 它们都支持直接访问数据和通过指定起止行键访问数据功能。读者可以在查询中添加更多的限制条件来减少查询得到的数据量，这些限制可以是指定列族、列、时间戳以及版本号。

这些方法可以帮助用户控制那些数据在查询时被包含其中，但它们缺少一些细粒度的筛选功能，比如基于正则表达式对行键或值进行筛选。 Get 和 Scan 两个类都支持过滤器，过滤器最基本的接口叫 Filter，除此之外，还有一些由 HBase 提供的无需编程就可以直接使用的类。

同时用户还可以通过继承 Filter 类来实现自己的需求，所有的过滤器都在服务端生效，这样可以保证被过滤掉的数据不会被传送，可以在客户端代码实现过滤的功能，因为在这种情况下服务器端需要传输更多的数据到客户端，用户应当尽量避免这种情况。

1.客户端创建 Scan 过滤器
2.发送过滤器数据的序列化 Scan
3.RegionServer 使用过滤器对 Scan 进行序列化，并同时使用 Scan 和内部的扫描器

1. 过滤器层次结构

在过滤器层次结构的最底层是 Filter 接口和 FilterBase 抽象类，它们实现了过滤器的空壳和骨架，这使得实际的过滤器可以避免许多重复的结构代码。

大部分实体过滤器类一般都直接继承自 FilterBase，也有一些间接继承自该类。不过它们的使用流程相同，用户定义一个所需要的过滤器实例，同时把定义好的过滤器实例传递给 Get 或 Scan 实例:
setFilter(filter)

在实例化过滤器的时候，用户需要提供一些参数来设定过滤器的用途。其中一组特殊的过滤器，它们继承自 CompareFilter，需要用户同时提供至少两个特定的参数，这两个参数会被基类用于执行它的任务。

2. 比较运算符

因为要继承自 CompareFilter 的过滤器比基类 FilterBase 多了一个 compare() 方法，它需要摔死传入参数定义比较操作的过程，可用值

表4-1 CompareFilter 中的比较运算符

LESS		匹配小于设定值得值
LESS_OR_EQUAL	
EQUAL
NOT_EQUAL
GREATER_OR_EQUAL
GREATER
NO_OP
当过滤器被应用时，比较运算符决定什么被包含，什么被排除，这样帮助用户筛选数据的一段子集或一些特定数据

3. 比较器

CompareFilter 所需要的第二类类型是比较器（comparator），比较器提供了多种方法比较不同的键值。比较器都继承自 WritableByteArrayComparable,WritableByteArrayComparable实现了 Writable 和 Comparable 接口。如果想使用HBase 原生提供的比较器则可以，这些比较器构造时通常只需提供一个阈值，这个值将会与表中的实际值进行比较：

表 4-2 HBase 对基于 CompareFilter 的过滤器提供的比较器

BinaryComparator	使用 Bytes.compareTo() 比较当前值与阈值
BinaryPrefixComparator	使用Bytes.compareTo() 匹配，但是是从左端开启前缀匹配

NullComparator		不做匹配，只判断当前值是不是 null
BitComparator		通过 BitwiseOp 类提供的按位与、或、异或操作执行位级比较

RegexStringComparator	根据一个正则表达式，在实例化这个比较器时去匹配表中的数据

SubstringComparator		把阈值和表中数据当作 String 实例，同时通过contains() 操作匹配字符串

通常每个比较器都有一个带比较值参数的构造函数，用户需要定义一个值来跟每个单元格做比较。一些构造函数使用字节数组作为参数，并通过二进制进行比较，还有一些使用String 参数（）比较。

4.1.2 比较过滤器

HBase 提供的第一种过滤器实现是比较过滤器。用户创建实例时需要一个比较运算符和一个比较器实例。每个比较过滤器的构造方法都有一个从 CompareFilter 继承的签名方法。

CompareFilter(CompareOp valueCompareOp,
	WritableByteArrayComparable valueComparator)

用户需要提供比较运算符和比较类让过滤器工作，
HBase 过滤器本来是为了筛选无用信息，被过滤掉的信息不回被传送到客户端，过滤器不能用来指定用户需要哪些信息，而是在读取数据的过程中不返回用户不想要的信息。
正好相反，所有基于 CompareFilter 的过滤处理过程与上面所描述的恰好相反，它们返回匹配的值。用户需要根据过滤器的不同规则来挑选过滤器。

1. 行过滤器（RowFilter）

行过滤器基于行键 过滤数据，通过使用不同过滤器来获取需要的行，使用多种比较运算符来返回复合条件的行键，同时会过滤不符合条件的行键，

2. 列族过滤器（FamilyFilter）

这个过滤器与行过滤器

3. 列名过滤器（QualifierFilter）

4.

5. 参考列过滤器（DependentColumnFilter）

这种过滤器不仅简单地通过用户指定的信息筛选数据。允许用户指定一个参考列或是引用列，并使用参考列控制其他列的过滤。参考列过滤器使用参考列的时间戳，并在过滤时包括所有与引用时间戳相同的列。：
DependentColumnFilter(byte[] family,byte[] qualifier)
DependentColumnFilter(byte[] family,byte[] qualifier,
	boolean dropDependentColumn)
DependeneColumnFilter(byte[] family,byte[] qualifier,
	boolean dropDependentColumn,CompareOp valueCompareOp,
	WritableByteArrayComparable valueComparator)

一个 ValueFilter 和一个时间戳组合，用户可以传入比较运算符和基准值来启用 ValueFilter 的功能。这个过滤器的构造函数默认允许用户在所有列上忽略运算符和比较器，以及屏蔽按值筛选功能，也就是说，整个过滤器只基于参考列的时间春进行筛选。
dropDependentColumn 参数帮助用户操作参考列：该参数设为false 或 true 决定了参考列可以被返回还是被丢弃。

这种过滤器与扫描操作的批量处理功能不兼容，需要使用 Scan.setBatch() 方法将 batch 值设为一个比 0 大的数，过滤器需要查看整行数据来决定哪些数据被过滤，使用批量处理可能会导致取到的数据不包括参考列，
如果启用批量处理模式，以下错误：
 IncompatibleFilter Exception:
 cantnot set batch on a scan using a filter that returns ture for filter.hasFilterRow

 4.1.3 专用过滤器

 HBase 提供的第二类过滤器直接继承自 FilterBase, 同时用于更特定的使用场景。其中的一些过滤器只能做行筛选，因此只适用于扫描操作，对 get() 方法来戳，这些过滤器限制得过于苛刻： 要么包括整行，要么什么都不包括。

1. 单列值过滤器（SingleColumnValueFilter）
用一列的值决定是否一行的数据被过滤，首先设定待检查的列，然后设置待检查的列的对应值。具体构造函数如下：
	SingleColumnValueFilter(byte[] family,byte[] qualifier,
		CompareOp compareOp,byte[] value)
	SingleColumnValueFilter(byte[] family,byte[] qualifier,
		CompareOp compareOp,WritableByteArrayComparable comparator)
第一个在内部创建一个 BinaryComparator 实例，第二个构造函数中所需参数与用户一基于 CompareFilter 类相同，

同时，过滤器提供了一些辅助方法帮助用户微调过滤行为。
	boolean getFilterIfMissing()
	void setFilterIfMissing(boolean filterIfMissing)
	boolean getLatestVersionOnly()
	void setLatestVersionOnly(booelan getLatestVersionOnly)
前者到参考默认这一行吧，使用 setFilterIfMissing(true) 来过滤这些行，在这样设置后，所有不包含参考列的行都被过滤掉。

	用户在扫描时必须包括参考列，用户使用类似于 addColumn() 方法把参看列添加到查询中，如果没有这么做，扫描结果没有包括参考列，那结果可能为空或包含所有行，

2. 单列排除过滤去

3. 前缀过滤器（prefixFilter)
	public PrefixFilter(byte[] prefix)

4. 分页过滤器（pageFilter）
对结果按行分页，需指定 pageSize 参数，控制每页返回的行数
在物理上奋力的服务器中并行执行过滤操作时，需注意：在不同的 region 服务器上并行执行的过滤器不能共享它们现在的状态和边界，因此，每个过滤器都会在完成扫描前获取 pageCount 行的结果， 这种情况使得分页过滤器可能失效，极有可能返回的比所需要额定多，最终客户端在合并结果时可以选择返回所有结果，也可以使用API 根据需要求筛选结果。

客户端两码记录本次扫描的最后一行，并在下一次获取数据时把记录的上次扫描最后一行设为这次扫描的起始行，同时保留相同的过滤属性，然后依次进行迭代。
分页是对依次返回的行数设定了严格的限制，一次扫描所覆盖的行数很可能是多于分页大小的，过滤器有一种机制通知 region 服务器停止扫描。

5. 行键过滤器（KeyOnlyFilter）
在一些应用中只需要将结果中KeyValue 实例的键返回，而不需要返回实际的数据，KeyOnlyfiltr 提供了可以修改扫描出的列和单元格的功能，这个过滤器通过KeyValue.convertToKeyOnly(boolean ) 方法帮助调用只返回键不返回值。
这个过滤器构造函数需要一个叫lenAsval 的布尔参数。这个参数会被传入 convertToKeyOnly() 方法中，它可以控制 KeyValue 实例中值得处理。默认值为false,设置为 false 时，值被设为长度为0的字节数组，设置为 true 时，值被设为原值长度的字节数组。

键虽然包含了有意义的信息，但值得长度可能用来做二次排序并需要快速迭代所有列，此时前段描述中的后者对当前的应用非常有用。

6. 首次行键过滤器（FirstKeyOnlyFilter）
rugo ys湖需要访问一行中的第一列，则这种过滤器可以满足需求。这种过滤器通常在行数统计的应用场景中使用，这种场景在列式存储数据库中如果某一行存在，则行中必然有列。
由于列也按字典序排列，因此其他可能用到的场景是按照时间先后生成列名，这样最旧的列就会排在最前面，因此时间戳最久的列会最先被检索到。这个场景与当前的过滤器结合使用，通过单一的扫描操作金额可以得到每行中最早创建的列。
这个类使用了过滤器框架提供的另一个优化特性：它在检查完第一列之后会通知 region 服务器结束对当前行的扫描，并调到下一行，与全表扫描相比，器性能得到提升。

7. 包含结束的过滤器（InclusiveStopFilter）
扫描操作中的开始行被包含到结果中，但终止行被排除在外，使用这个过滤器，用户也可以将结束行包括在结果中。

8. 时间戳过滤器（TimestampsFilter）
当用户需要在扫描结果中对版本进行细粒度控制，用户需要传入一个装载了时间戳的List 实例的元信息：
TimestampsFilter(List<long> timestamps)
一个版本（version) 是指一个列在一个特定时间的值，因此用一个时间戳来表示。当过滤器请求一系列的时间戳时，它会找到与其中时间戳精确匹配的列版本

9. 列计数过滤器（ColumnCountGetFilter）
限制每行最多取回多少列。使用以下构造器设置：
ColumnCountGetFilter(int n)
适合在 get() 方法中使用 

10. 列分页过滤器（ColumnPaginationFilter）
与PageFilter相似，这个过滤器可以对一行的所有列进行分页，构造器需要两个参数：
ColumnPaginatioinFilter(int limit,int offset)
它将跳过所有偏移量小于 offset 的列，并包括之后所有偏移量在 limit 之前(包含 limit) 的列

11. 列前缀过滤器（ColumnPrefixFilter)

4.1.5 FilterList

使用以下构造器创建 FilterList 实例时，提供多种不同参数：
	FilterList(List<Filter> rowFilters)
	FilterList(Operator operator)
	FilterList(Operator operator,List<Filter> rowFilters)

参数 rowFilters 以列表的形式组合过滤器，参数 operator 决定了组合它们的结果。表提供了一些可选的操作符，默认值是 MUST_PASS_ALL，当用户不许其他操作符时可以在构造器中省略该参数。
MUST_PASS_ALL 
MUST_PASS_ONE

当创建void addFilter(Filter filter)

4.2 计数器

客户端API 提供了专门的方法来完成这种读取并修改操作，同时在单独一次客户端的调用过程中保证原子性。早起的HBase 版本只会在每次计数器更新操作中使用RPC 请求，不过新版HBase 中 CRUD 操作开始使用与此相同的机制，让许多更新计数器的请求都可以在已 RPC 中完成。
每次 incr 调用返回这个计数器的新值，最后检查时使用了 get_counter，并显示当前

4.4 HTablePool

HTableInterfaceFactory 接口
用户创建zidi工厂类，，如果用户想自己扩展 HTableInterfaceFactory，则必须实现下面两个方法：
 HTableInterface createHTableInterface(Configuration config,byte[] tableName)
 void releaseHTableInterface(HTableInterface table)
 第一个方法用于创建 HTable 实例，第二个方法用于释放实例，用户可以在调用前准备好实例，并在使用完成之后进行一些相应的清除操作。用户在共享表引用时对客户端写缓冲区的处理。 releaseHTableInterface() 是完成一些隐式操作的理想方法，比如写缓冲区刷写、调用 flushCommits() 请求来执行。
 工厂类有个默认实现HTableFactory，当工厂调用 create 方法时创建 HTable 实例，当调用 release 方法时调用 HTabel.close()。
 如果用户不指定自己的工厂类，系统会默认使用 HTableFactory.
 可以用如下调用方式来使用表实例池：
 HTableInterface gettable（String tableName)
 HTableInterface gettable(byte[] tableName)
 void putTable(HTableInterface table)
 gettable() 从表实例池中获取 HTable 实例，使用之后通过 putTable() 方法放回，以上两种方法把一些工作迁移到表实例池配置的 HTableInterfaceFactory 接口

maxSize 参数仅仅设置表实例池中能够存放的 HTableInterface 实例的数目。例如，当用户将这个值设置为5 时，调用 10 此 getTable() 会创建 10 个HTable 实例。不过之后只有 5 次 putTable() 方法发挥作用，后面5 次直接忽略，
关闭表实例池中特定表实例的方法：
 void closeTablePool(String tableName)
 void colseTablePool(byte[] tableName)

 close 方法遍历所有保存在列表中与参数对应的表引用，然后使用工厂的释放机制。这对于释放一张表的所有资源，并重头再来非常有用。

 使用场景： Hush
 Hush 中所有的表都是使用表实例池来获取，下面是使用表实例池共享表实例的代码。


4.5 连接管理
每个 HTable 实例都需要建立和远程主机ed连接，这些连接在内部使用 HConnection 类表示，更重要的是，其被 HConnectionManager 类管理并共享。用户没有必要同时和这两个类打交道，只需要创建一个 Configuration 实例，然后利用客户端 API 使用这些类。
HBase 内部使用键值映射来存储连接，使用 Configuration 实例作为键值映射的键。换句话说，当你创建很多 HTable 实例时，如果你提供了相同的 Configuration 引用，那么它们都共享同一底层的 HConnection 实例。
共享 Zookeeper 连接
	因为每个客户端最终都需要 Zookeeper 连接来完成表的 region 地址初始寻址。连接一旦建立，共享这使得之后的客户端实例可以共用。
缓存通用资源
	通过 Zookeeper 查询到的 -ROOT- 和。META。的地址，以及 region 的地址定位都需要网络传输开销，这些地址将被缓存在客户端来减少网络的调用此处，达到加速寻址的目的。
	对于每个链接到远程集群的本都客户端来说，它们的地址表都是相同的，因此运行相同进程的客户端共享连接非常有用，这是通过共享 HConnection 实例来实现的。另外，当寻址失败时（如 region 拆分时），连接有内置的重试机制刷新缓存，对于其他所有共享相同连接引用的客户端来说，这项更改立即生效，因此这更加减少了客户端初始化连接的开销。
	另一个受益的类是 HTablePOol，所有连接池中的 HTable 实例都自动共用一个提供的 Configuration 实例，因此它们共享连接，当用户想创建多个 HTable 实例时，最好先创建一个共用的 Configuration 实例。

共享连接的缺点：用户可以通过显式关闭连接来避免这种情况，建议用户不再需要 HTable 时主动调用 HTable() 的close() 方法，调用这个方法将释放所有共享资源，其中包括 Zookeeper 连接，同时移除内部列表中的连接引用。

每次用户重用 Configuration 实例时，连接管理器都会增加引用计数。因此用户必须调用 close() 来触发清除工作，以下是用显式的方法来清理一个连接或所有连接。
	static void deleteConnection(Configuration conf,boolean stopProxy)
	static void deleteAllConnections(boolean stopProxy)

所有的共享连接都是按照 Configuration 实例作为键，因此用户需要提供这个实例来关闭相应的连接。布尔类型参数 stopProxy 保证强制清除整个客户端的 RPC 栈，因此不再需要远程连接服务器时，应该将这个参数设置为 true。

如果用户需要显式地使用某个连接，可以通过如下方式使用 getConncetion() 方法
	Configuration newConfig = new Configuration(originalConf);
HConnection connection = HConnectionManager.getConnection(newConfig);
// Use the conncetion to your hearts' delight and then when doen..
HConnectionManager.deleteConnection(newConfig,true);

第 5 章		客户端API : 管理功能
除了进行数据处理的客户端API，

5.1 模式定义
在HBase 中建表涉及到表结构以及所有涉及的列族结构定义，

5.1.1 表
在HBase 中数据最终会存储在一张表或多张表中，使用表的主要原因是控制表中的所有列以达到共享表内的某些特性，一个典型的例子是定义表的列族，下面是表描述符的构造函数。

	HTableDescriptor();
	HTableDescriptor(String name);
	HTableDescriptor(byte[] name);
	HTableDescriptor(HTableDescriptor desc);

	Writable 和无参数的构造函数
API 提供的大多数类和本章中的类都拥有一个特殊的构造函数，即一个不带任何参数的构造函数。因为这些类都实现了 Hadoop Writable 接口。

任意不相交系统间的远程通信： 例如，客户端与服务器端进行交互，或者服务器端彼此进行内部通信，都使用到了 Hadoop RPC 框架，这个框架中需要远程方法中的参数都实现 Writable 接口，进而能序列化对象并进行远程传输。Writable 接口有两个必须实现的方法：
	void write(DataOutput out) throws IOException;
	void readFields(DataInput in) throws IOException;
框架通过调用这两个方法把对象序列化成输出流，通信接收端读取字节流，并将其反序列化成对象。框架在数据发送端调用 write() 方法，并序列化对象的字段，同时会在字节流中添加类名以及其他一些信息。
数据接收服务器先读取元数据信息，并创建类的无参数实例，然后调用新创建对象的 readFields() 方法，将字节流中的信息读取到对应对象的字段中，用户可以理解为这是数据发送端对象的一个可运行的，已经初始化的完整副本。
因为数据接收端首先读取元数据并创建该类的空实例，并通过 readFields() 方法及序列化字段信息到新创建的实例中。通常客户端与服务器端需使用相同的 HBase JAR。
如果用户开发并扩展了 HBase 的基础实现，例如，过滤器和协处理器，用户必须满足以下条件：
1. 在 RPC 通信两端都必须可用，即在数据发送进程与数据接收进程均可用。
2. 实现 Writable 接口，并实现了 write() 与 readFields() 方法
3. 拥有无参数的构造函数，即一个没有任何输入参数的构造函数。

如果运行时无法使用这个特殊的构造函数生成实例，则会报运行时错误，

用户通过表名或已有的表描述符来创建表。没有任何参数的构造函数仅仅为了反序列化，并且不应该被直接使用。表名通常使用 Java String 类型或 byte[] （二进制数组）表示，
表名作为存储系统中存储路径的一部分来使用，因此必须符合文件名规范，因此构成表名的字符是有限制的。用户可以直接查看低级别存储系统，例如，用户在 HDFS 中可以看懂

HBase 列式存储格式允许用户存储大量的信息到相同的表中。
虽然理论上 hBase 表是由行和列组成的，但是从物理结构看，表存储在不同的分区，即不同的 region，每个 region 只在一个 region 服务器中提供服务，而 region 直接向客户端提供存储服务。

5.1.2 表属性
表提供了 getter 与 setter 方法来设置表的其他属性。实际上，这些属性中的大部分都很少用到，用户可以使用这些方法来微调表的性能，因此用户需要了解他们。
名：
构造函数中已经有设置表名的参数， JavaAPI 也提供了显示设置表名的方法。

列族：
列族是表中非常重要的一部分，用户需要在表中指定将要使用的列族。
void addFamily(HColumnDescriptor family);
boolean hasFamily(byte[] c);
HColumnDescriptor[] getColumnFamilies();
HColumnDescriptor getFamily(byte[] column);
HColumnDescriptor removeFamily(byte[] column);
用户可以添加列族、通过列族名来检查列族是否存在、获取存在的列族的列表、获取某个列族描述符或删除某个列族等操作。
文件大小限制:
这个参数限制了表中 region 大小，显式获取和设置该参数的值：
long getMaxFileSize();
void setMaxFileSize(long maxFileSize);
	文件大小限制并不能表达其真正含义，其真正含义是每个存储单元大小的限制，即一个列族有若干个存储单元，而其中每个存储单元会包含若干文件。如果一个列族的存储单元已使用的存储空间超过了大小限制， region 将发生拆分操作，所以这个参数被称为 maxStoreSize 更合适。
	当region 大小达到配置大小时，文件大小限制会帮助 hBase 拆分 region，

只读：
默认所有的表都可写，但是，对一些特殊的表来说，只读参数有特殊的用途。如果该参数设置为 true,这个表只能读而不能修改数据。通过以下方法可获取和设置该参数：
	boolean isReadOnly();
	void setReadOnly(boolean readOnly);

memstore 刷写大小
	HBase 在内存中预留了写缓冲区，写操作会写入到写缓冲区，然后按照合适的条件顺序写入磁盘的一个新存储文件中，这个过程为刷写，这个参数能控制合适触发
示例方法：
	long getMemStoreFlushSize();
	void setMemStoreFlushSize(long memstoreFlushSize);

上面提到的文件大小限制跟需求相关，该参数的默认值是 64 MB,该参数越大，可以生成的存储文件越大，文件数量会越少，同时也会导致更长的阻塞时间问题，这种情况， region 服务器不能持续接收新增加的数据，请求被阻塞的时间随之增加，此外，一旦服务器崩溃，系统通过 WAL 恢复数据的时间增加，且更新的内存丢失。

延时日志刷写：
HBase有两种将WAL 保存到磁盘的方式，一种是延时日志刷写
这个参数是一个布尔型，默认false，下面通过 Java API 模式设置参数：
synchronized boolean isDeferredLogFlush();
void setDeferredLogFlush(boolean isDeferredLogFulsh);

其他选项：
	除了已经提到的属性，还有提供给用户设置任意键值对的方法：
	byte[] getValue(byte[] key)
	String getValue(Stirng key)
	Map<ImmutableBytesWritable,ImmutableBytesWritable> getValue()
	void setValue(byte[] key,byte[] value)
	void setValue(String key,String value)
	void remove(byte[] key)
以上方法中的数据均存储在预定义的表中，

5.1.3 列族
通过 HTableDescriptor 为一张表增加列族，于此类似，
列族定义了所有列的共享信息，可以通过客户端创建任意数量的列，列常用的变量名为 column qualifiers。定位某一具体列需要列族名与列名合并在一起，以 ":"
分割： family:qualifier

用户需要注意空列明的使用，也可以只使用列族名而忽略列名，读写操作与其他列无区别，但是最好增加列名来甲乙区分。
用户可以不
名字：每个列族通过 HColumnDescriptor 实例的以下方法获取列族名。
	byte[] getName();
	String getNameAsString();
	列族不能被重命名，通常做法是仙剑一个列族，使用API 从旧列族中复制数据到新列族。
除了通过构造函数，没有其他重命名列族的途径，列族名必须是可见字符。
最大版本数：

压缩：
压缩类型是 Compression.Algorithm 枚举，枚举中类型：NONE，GZ,LZO，SNAPPY

块大小：

5.2 HBaseAdmin
客户端提供 API 的模式管理集群，与 RDBMS 中的 DDL 相比---客户端提供的具有管理功能的 API 更像 DML。

HBaseAdmin 提供建表、创建列族、检查表是否存在、修改表结构和列族结构和删除表等功能。

5.2.1 基本操作

HBaseAdmin(Configuration conf) throws MasterNotRunningException,ZooKeeperConnectionException
	管理功能 API 大多抛出 IOException，或者 InterruptedException，前者是客户端与服务器端的通信异常，后者是执行过程中的干扰异常，如region 服务器的执行命令在完成前被停止所引起的问题

已有的配置实例提供了足够的配置信息，所有当前的 API 可以通过使用 Zookeeper相关配置信息查找集群，类似于普通客户端 APi 使用方法，具有管理功能的 APi 实例应该在使用后销毁，这个实例不应长期保留。
 HbaseAdmin 实例声明周期不宜太长，例如在 master 故障恢复的过程中，它是短暂有效的。

HBaseAdmin 实现了 Abortable 接口，并实现了以下方法：
 void abort(String why,Throwable e)
以上方法被框架隐式调用，例如，当发生致命连接错误或关闭集群时，用户不能直接调用这个方法，但是在紧急情况下，例如需要完整关机或重启时，系统会调用该方法。

以下方法可以获得 master 的远程对象：
	HMasterInterface getMaster() throws MasterNotRunningException,
	ZooKeeperConnectionException
以上方法会返回 HMasterInterface 接口的 RPC 代理实例，允许用户直接通过这个实例访问 master 服务器。不过这个方法不是必须，HBaseAdmin 内置了 master 所有 RPC 接口代理的封装。
	除非用户确定自身的调用是安全的，否则不要直接调用 getMaster() 获取 HMasterInterface 远程对象，HBaseAdmin 自身已经封装了 HMasterInterface 远程对象的调用功能，例如，检查输入是否合法，并转化成远程异常返回，或优化同步处理为异步处理以提升执行能力。

HBaseAdmin 类还提供一下基本接口：
	boolean isMasterRunning()
	通过这个接口检查

	HConnection getConnection()

5.2.2 表操作
建表：
 void createTable(HTableDescriptor desc)
 void createTable(HTableDescriptor desc,byte[] startKey,byte[] endKey,
 int numRegions)
 void createTable(HTableDescriptor desc,byte[] splitKeys)
 void createTableAsync(HTableDescriptor desc,byte[][] splitKeys)

createTable(HTableDescriptor desc,byte[] startKey,byte[] endKey,int numRegions) 能以特定数量拆分特定起始行键和特定终止行键，并创建表。参数 startKey 必须小于 endKey，并且 numRegions 需大于等于3,region 边界是通过终止行键减去起始行键然后除以给定的 region  数量计算得到，在上面例子中，用户能够
表的大多数管理功能时异步的，这对发送命令而不需要等待执行结果的场景有用，但是，也有一些操作必须等待并获知操作执行成功后，才可以继续执行其他操作，因此，表操作提供了异步与同步两种操作模式。
实际上，同步模式仅仅是异步模式的简单封装，增加了不断检查这个任务是否已经完成的循环操作，例如， createTable() 方法包装了 createTableAsync()方法，循环检查远程服务器的建表操作是否已经执行完成。

建表后用户通过
boolean tableExists(String tableName)
boolean tableExists(byte[] tableName)
HTableDescriptor[] listTables()
HTableDescriptor getTableDescriptor(byte[] tableName)

用户在删除表时，需要确认这张表是否已经被禁用，region fuwuqi 
用户将表设置为禁用可能会花费非常长の时间，这取决于在服务器内存中有多少近期更新的数据还没有写入磁盘。将一个 region 下线会先将内存中的数据写入磁盘，如果用户设置了较大的堆，这将导致 region 服务器需要向磁盘写入大量数据，在负载很高的系统中进行数据写入时，多个进程间的竞争会使这个操作的执行时间变长。

重新启用该表：
void enableTable(String talbeName)

这些操作在转移这张表的 region 到其他可以服务器时有用，此外，以下是检查表的状态的一组方法：
	boolean isTableEnabled(String tableName)
	boolean isTableEnabled(byte[] tableName)

用户建表后，如果需要修改表结构必须删除表结构然后重建表，或采用如下方法改变表结构：
void modifyTable(byte[] tableName,HTableDescriptor htd)

void compact(String tableNameOrRegionName)
void compact(byte[] tableNameOrRegionName)
这个方法是个异步方法，因为合并是个花费时间较长的操作，客户端没有必要一致等待这个操作完成，调用这个方法后，这个表或 region 会加入到这个 region 所在的 region 服务器的一个执行队列，并会在后台完成操作，或在上线该表 region 的所有 region 服务器中执行。

void majorCompact(String tableNameOrRegionName)
void majorCompact(bytep[] tableNameOrReionName)
	以上方法类似 compact() 方法，也依赖后台队列操作，不过是执行的 major 合并，提供了表名后，API 内部会迭代这张表所有的 region,并顺序调用 region 
	的合并操作。
void split(String tableNameOrRegionName)
void split(byte[] tableNameOrRegionName)
void split(String tableNameOrRegionName,String splitPoint)
void split(byte[] tableNameOrRegionName,byte[] splitPoint)
	这个方法用于拆分一个 region 或拆分整张表，如果提供了表名，API 内部会迭代这张表所有 region 并调用拆分命令
	splitPoint这个参数不为空，并制定了特定的 region ，这个 region 会按照这个制定的行键来拆分，如果指定的是表名，整张表 的 region 在执行拆分前会进行检查，且包含这个特定行键的 region 会按照这个特定的行键进行拆分。
	如果使用 splitPoint，用户首先确保行键合法，它必须在给定的 region 中，并且必须大于 region 的起始行键，因为在一个 region 的起始行键处进行拆分无效，如果提供行键不正确，这个行键会被忽略，并且客户端没有任何反馈，但不熟这个 region 的 region 服务器会在日志中打印如下信息：
	Split rows is not inside region key range or is equal to startkey

void move(byte[] encodedRegionName,byte[] destServerName)
	使用move() 方法可以通过客户端控制某个 region 在哪台服务器上线，用户可以使用此方法将陈e reiong 从当前的 region 服务器移动到一个新的 region 服务器，destServerName 参数可以设置为 null，这样会获得一个随机的服务器地址，否则必须获取一个合法的服务器地址，即一个正在运行的 region 服务器进程，如果这个参数有误或这个服务器当前没有响应，这个 region 会在其他服务器上线。最糟糕的情况： 移动 region 失败，并让这个 region 下线。

boolean balanceSwitch(boolean b)
boolean balancer()
	第一个方法可以控制 region 的负载均衡算法是否开启，如果负载均衡算法已经打开，balancer() 能主动运行负载均衡算法将每台 region 服务器上线的 region 进行均匀再分配。

void shutdown()
void stopMaster()
void stopReginServer(String hostnamePort)
	这些方法分别关闭集群、关闭master进程和关闭某台 region 服务器操作，一旦调用，并执行的服务器马上进入关闭状态，即不存在延时吗，且不可逆。

5.2.5 集群状态信息
调用 HBaseAdmin.getClusterStatus() 可以查询 ClusterStatus 实例，这个实例包含 master 搜集到的整个集群信息，这个类页游 setter 方法，次方法允许用户修改里面的信息，但通过 set 方法修改得仅仅是本地副本的变量，除非用户需要修改本地副本的变量值。
表 5-4 ClusterStatus 提供的所有方法
int getServersSize()	当前活着的 region 服务器数量，此数量不包括不可用状态的 region 服务器
Collection<ServerName> getServers()		当前存活的region 服务器列表，包括 region 服务器的服务、IP、RPC端口、启动时间戳等

int getDeadServers() 	当前处于不可用状态的 region 服务器列表，包括 region 服务器的服务 ip、RPC 端口等

double getAverageLoad()	 平均每台 region 服务器上线多少 region，该方法类似于 getRegionsCount()

int getRegionsCount()	集群中 region 总数量

int getRequestsCount()	集群的请求 TimeStamp

String getHBaseVersion()	返回当前集群的软件编译版本

byte getVersion()	返回 ClusterStatus 实例的版本号，通过序列化的方式在 RPC 阶段传输

String getClusterId()	返回集群的唯一标识，这个值是集群第一次启动时 通过 UUID 生成的，存在根目录下的 hbase.id 中

Map<String,RegionState> getRegionsInTranstition()	返回当前集群正在进行处理的 region 的事务列表，即移动操作、上线操作和下线操作。键是编码后的region 名（由 HRegToInfo.getEncodeName()返回），值是 RegionState 实例

HServerLoad getLoad(ServerName sn) 	返回给定 region 服务器当前负载状况

用户通过查看集群状态可以在较高层次看到集群整体情况，查看使用 getServers() 方法返回的 ServerName 实例获取所有当前处于可用状态的服务器实际信息。

ServerName 提供信息
String getHostname()	返回服务器域名，如果域名不可用直接返回 IP 地址

String getHostAndPort()	返回域名与 RPC 端口的合并字符串，用“：”分隔，例如，<hostname>:<rpc-port>
long getStartcode()	服务器启动时间，单位毫秒，使用

String getServerName()	获取服务器名，服务器名是 <hostname>、<rpc-port>和 <start-code> 的组合
int getPort()	返回服务器端的 RPC 端口

通过提供 HServerLoad 实例，每台 region 服务器可以提供它们的负载信息。用户调用 ClusterStatus 实例的 getLoad() 方法可以获取 HServerLoad 实例。使用上面提到的 ServerName，通过 getServers() 方法可以获取并迭代访问服务器信息，而当前提到的 HServerLoad 不仅提供服务器本身信息，还提供每台服务器管理的 region 信息，表5-6 列出了所有可用方法。

表5-6  HServerLoad 提供信息概览
byte getVersion()	返回HServerLoad 版本号，RPC 序列化进程会用到这个参数

int getLoad()	等同于 getNumberOfRegions() 返回值

int getNumberOfRegions()	当前 region 服务器上线的 region 数量

int getNumberOfRequests()	参数 hbase.regionserver.msginterval 来设定。请求数会在一个周期结束后清零，它会统计所有的API 请求，如 get、put、increment、delete等

int getUsedHeapMB()		JVM	已使用的内存，单位为 MB

int getMaxHeapMB()	JVM最大可用内存，单位MB
int getStorefiles()	当前 region 服务器的存储文件数量，即包括这个服务器管理的所有 region

int getStorefilesSizeInMB()	当前region 服务器的存储文件的总存储量，单位MB
int getStorefileIndexSizeInMB()	当前 region 服务器的存储文件的索引大小，单位 MB
int getStorefileIndexSizeInMB()	当前
int getMemStoreSizeInMB()	当前 region 服务器已用写缓存大小，包括这个 region 服务器上所有的 region

Map<byte[],RegionLoad> getRegionsLoad()	返回当前 region 服务器中每个 region 负载情况，以 Map 形式返回，键是 region 名，值是之前讨论过的 RegionLoad 实例

RegionLoad 信息概览
byte[] getName()	返回 region 名，以原始二进制 byte 数组格式返回
String getNameAsString()	将二进制 region 名转换为字符串并返回
int getStores()	当前 region 的列族数量
int getStorefiles()	当前 region 存储文件数量
int getStorefileSizeMB()	当前 region 的存储文件占用空间，MB
int getStorefileIndexSizeMB()	当前 region 的存储文件的索引信息大小，MB
int getMemStoreSizeMB()	当前 region 使用的 MemStore 大小，单位MB
long getRequestsCount()	当前 region 使用的 

6.3 批处理客户端
另一种客户端的交互使用场景是批量访问数据。不同之处是这些批量处理通常是异步运行在后台，需要扫描大量数据，例如，扫描索引、基于数学模型的机器学习或报表统计需求。
这些处理案例大多不是用户直接驱动，而且所需运行时间非常漫长，因此用户不会特别关注单次访问的延时。大多数批量读写 HBase 的框架是基于 MapReduce 模式。
6.3.1 原生 Java
关于基于 MapReduce Java API	的访问

2. Clojure	

6.3.2 Hive
提供了基于 Hadoop 的数据仓库，Hive 最早由 FaceBook 开发
hive 提供了类似 SQL 的处理语言，叫 HiveQL，允许用户查询存储在 Hadoop 中的半结构化数据。最终查询会转化 Mapreduce 作业，在本地执行或在分布式 Mapreduce 集群中执行。数据在作业执行的时候被解析，并且 hive 提供了一个不仅可访问 hdfs 的数据还可以访问其他数据源的存储处理层，存储层的数据获取对用户查询是透明的。
6.4 Shell
HBase Shell 是集群的命令行接口，用户可以使用 shell 访问本地货远程服务器并与其交互，shell 同时提供了客户端和管理功能的操作。
6.4.1 基础
启动 shell:
hbase shell
用户可以通过在调用时添加命令来请求特定的帮助，还可以对一组命令请求帮助，命令或组名都需要引号括起来

第七章：与MapReduce 集成

7.1 框架
7.1.1 
Mpareduce 被设计为在可扩展方式下解决 TB 级数据处理问题，有一种方法可以建立在一个性能随机器数增加而线性提升的系统，这就是 mapreduce，分支，
首先，它会可靠地将输入数据拆分成大小合理的块，然后服务器每次处理一个块。一般来说，拆分工作需尽可能利用可用的服务器和基础设施。图示数据可以是非常大的日志文件，处理时被划分为大小相等的分支，这样做非常适合处理如 apache 日志类型文件。输入的数据也可以是二进制文件，但是此时用户需实现自己的 getSplits() 方法，同时也有可能涉及：

7.1.2 类
1. InputFormat
第一个接触到的类是 InputFormat，它负责两件事情： 第一件是拆分输入数据，同时返回一个 RecordReader 实例，这个实例定义了键值对象的类，并提供了 next() 方法来遍历输入的数据。
HBase 而言，它提供了一组专用实现，叫做 TableInputFormatBase，该实现的子类是 TableInputFormat，

这些类提供一个扫描 HBase 全表的实现，用户需要提供一个 Scan 实例，设置起止行键、添加过滤器和指定版本数目等。TableInputFormat 将表拆分成大小合适的块，同时交给后面的 mapreduce 过程处理，

2. Mapper
Mapper 类构成了Mapreduce 过程下一阶段，同时也是其命名的原因之一，这一阶段，从 recordReader 读到的每一个数据都由一个 map() 方法处理，Mapper 读取一组特定的键/值对，但是可能会输出其他的类型，这个过程非常便于将原始数据转化为更有用的数据类型，以做进一步处理。

4. OutputFormat
7.1.3 支撑类
Mapreduce 的支撑类与 TableMapReduceUtil 类一同协作在 HBse 上执行 Mapreduce 作业，它有一个静态方法能配置作业，并使作业可以使用 HBase 作为数据源或目标。

7.1.4 MapReduce 执行地点
Hadoop 块备份策略对生层用户十分不透明：这个过程是自动完成的，用户不用为其操心。 HBase 依靠这个副本策略对上层提供持久化存储，虽然这些对于上层完全透明，但这些会如何影响系统性能。
不论是基于 HBas 还是 Hadoop，这个问题都是用户开始使用 MapReduce 作业时会产生的疑问，特别是 当大量数据存储在 HBase 中时，系统如何将数据存放在需要它的地方？ 这些都要参考数据的位置以及 HBase 如何使用 Hadoop 文件系统（HDFS）
首先，MapReduce 文档指出任务在靠近其数据的地方执行，为了达到这一点，HDFS 中的大文件被切分成一些小的块，默认大小是 64MB。
每块的数据被指定到一个 map 任务处理，这也说明较大的块会减少所需 map 数目，因为 map 数目是由需要处理的块的数目决定。 Hadoop 知道块的位置，所以会直接在其对应位置上运行 map 任务（实际上由于每个块都有若干副本，副本数量默认3，框架可以在这3个中任意选择一个执行，用于负载均衡），这是 MapReduce 保证数据本地计算的方法。
回到 HBase，当用户明白这一点后，可能会对这种机制如何在 HBase 上起作用产生疑问。 HBase 将数据存储在 HDFS 上。存储过程对于用户数来说是透明的，HBase 通过数据文件 HFile 和它的日志文件（WAL）来实现，如果用户查看代码，会发现它适用 Hadoop API 方法 FileSystem.create(Path path)来创建这些文件。
如果用户的 Hadoop 和 HBase 没有共用集群，而是彼此分开且相互独立，此时相当于运行一个 MapReduce 集群，作业不能再相应的 datanode 上执行，如果需要任务在数据所在位置执行则必须提供相应的数据位置信息，所以这种情况下 HDFS ，MapReduce 和 HBase 必须使用同一套集群。
Hadoop 如何知道 HBase 中数据分布？最重要的是 HBase 没有被频繁重启，同时日常维护操作都在正常按时运行，当有新数据加入时，合并会重写数据文件。所有 HDFS 上的文件都写入之后都是不可修改得，因此，数据将被写入新文件，而新文件数目也会增加，HBase 会将多个文件合并为一个新文件。
此外， HDFS 可以根据数据的使用位置智能地放置数据副本，它自带一个副本放置策略，该策略可以在写入数据时先让数据写入到辅助定位服务器，收到数据的节点先比较自己的服务器名称和写数据的程序所在的机器是不是一样，如果一样就将数据写入本地磁盘，然后数据的一个副本被送到
如果用户配置了一个更大的副本数目，更多的副本会存在不同的机器上，当BHase 的 region 服务器长时间没有重启，表经过 major 合并之后（major 合并可以由用户手动触发或由系统配置定时运行），表的数据就都会由一份本地副本，本地的 Data Node 也会有 region 服务器上所需数据的本地副本。如果用户 运行 scan 和 get 或其他操作，用户会得到更好的性能。
region 在负载均衡或服务器故障时可能会出现移动的情况，数据可能不在本地，但随着时间推移，其可能会再次移到到数据所在地， master 在集群重启时：会把 region 分配到它的原属 region 服务器上，只有当原属服务器不存在时才进行随机分配。
7.1.5 表拆分
当用户使用 Mapreduce作业从表中读取数据时，实质上使用 TableInputFormat 取得数据，它符合 Mapreduce 框架，并重载公有方法 getSplits() 和 create RecordReader()。 在一个作业呗执行前，框架调用 getSplit() 来决定如何划分块，从而由块数量决定映射任务的数目。
对于 HBase 来说，TableInputFormat 基于用户提供的 scan 实例取得所需的表信息，并且按 region 来划分边界。由于它不能直接地预测到可选过滤器的执行效果，所以它简单地使用起止键来确定 region 。拆分块的数目与起止键之间的 region 数目相等。如果用户满意设置这两个键，则所有 region 都包含在其中。
作业启动时，框架会按拆分的数目调用 createRecordReader()，并返回与当前块对应的 TableRecordRreader 实例，换句话说，每个 TableRecordReader 实例处理一个对应的 region，读取并遍历一个 region 所有行。
每一个拆分也包含对应的 region 所在服务器信息，正式这个能使 HBase 上的 mapreduce 作业本地化执行：框架会比较 region 信息对应的服务器名，如果有任务 tracker 在对应服务器上运行，它会挑选这个服务器来执行作业。因为 region 服务器与 DataNode 运行在一个节点上，所以扫描会从本地磁盘读取文件。
当在 HBase 上运行 mapreduce 时，推荐用户关闭预测执行模式，这样会增加 region 和服务器的负载，同时与本地化运行相违背： 预测作业在一个不同机器上运行，因此没有本地 region 服务器，这会增加网络带宽开销。

7.2 在 HBase 之上的 MapReduce
7.2.1 准备
当运行 mapreduce 作业所需库中的类不是绑定在 Hadoop 或 mapreduce 框架中时，用户就必须确保这些库在作业执行前已经可用，用户一般两个选择：在所有任务节点上准备静态库，或直接提供作业所需的所有库。
1. 静态配置
对于经常使用的库来说，最好将这些 jar 文件安装在 mapreduce 任务 tracker 的本地，可以通过以下步骤来完成。
1. 把 jar 文件复制到所有节点的常用路径中。
2. 将这些 jar 文件的完整路径写入 hadoop-env.sh 配置文件中，按照如下方式编辑 Hadoop_classpath 变量：
	# Extra Java Classpath elements. Optional.
	# export hadoop_calsspath = "<extra_entries>:$hadoop_classpath"
3. 重启所有任务 tracker 使变更生效
export hadoop_classpth = "$HBASE_HOME/hbase-0.91.0-SNAPSHOT.jar:\
$ZK_HOME/zookeeper-3.3.2.jar:$HADOOP_CLASSPATH"
这里的前提是假设用户已经定义了两个 ￥XYZ_HOME 环境变量，并在环境变量中之处软件安装路径。
这将可以修复
2. 动态配置
在一些情况下，用户希望每个他们运行的作业可以使用不同的库，或者在作业运行时能够更新库，这种情况下动态配置就
hadoop 有一个特殊功能：它可以读取操作目录下/lib 目录下包含的所有库的 jar 文件，用户可以使用此功能生成所谓的 “胖" jar 文件，因为他们传输的不仅仅作业代码，还有所有需要的库，这意味着作业 jar 文件更大，但从另一方面来说，其中已经自己包含了处理作业的完整代码。
使用Maven不仅可以帮助用户编译示例程序，还可以编辑加强的"胖" jar 文件并将其部署到 mapreduce 框架中，这避免了编辑服务器端的配置文件。
maven 已经支持了 profiles，这可以帮助 用户自定义编译过程，本章的 pom.xml 利用这个功能添加了一个 fatjar 配置，并将 /lib 目录下所有的所有库文件打包到一个最终的作业 jar 中。在这个工作属性中，一些依赖可以将 scope 属性定义为 provided，以使这些文件不被包含在上述复制过程中，当一些库在服务器中已经可用时，可以适当地增加这样的标记，例如，hadoop的 jar：
<dependency>
	<groupId>org.apache.hadoop</groupId>
	<artifactId>hadoop-core</artifactId>
	<version>0.20-append-r1044525</version>
	<scope>provided</scope>
</dependency>
本书资料库的根目录下的父 POM 文件定义了上述信息，此外，本章 POM 中定义依赖的地方也包含了此信息。其中一个例子是 apache commons cli 库，该库已经是 hadoop 一部分。
胖 jar 配置使用了 maven assembly 插件，在 src/main/assembly/job.xml 文件中配置了哪些文件应该被包含，哪些文件不应该被包含在目标 jar 文件中，为取代现有配置，用户可以编译一个 “瘦 jar,即一个只包含作业类，且需要更新服务器端配置来包含 hbase 和zookeeper 的jar。
mvn package
能编译一个被 mapreduce 框架执行的jar，运行时可以使用如下命令：
hadoop jar target/hbase-book-ch07-1.0.jar
mvn package -Dfatjar
生成的 jar 文件附加了一个后缀来彼此区分，但这仅仅是一种尝试
另一种动态提供必备库的方式是 hadoop mapreduce 框架的 libjars 功能，当用户使用 genericOptionsParser 创建一个 mapreduce 作业时，可以得到 libjar参数提供的支持。以下是该参数类的文档：

7.2.2 数据流向
使用 HBase作为数据流向，这个例子是在 tableOutputFormat 类的帮助下实现的。

create 'testtable','data'

$ hadoop dfs -put /projects/private/hbase-book-code/ch07/test-data.txt
$ hadoop jar target/hbase-book-ch07-1.0-job.jar ImportFromFile \
-t testtable -i test-data.txt -c data:json
第一个命令是 hadoop dfs -put,该命令将样本数据集合存储到hdfs 用户目录中，第二个命令则可以运行这个作业，作业的完成需要一段时间，使用默认的 TextInputFormat 类读取数据，这个类由 hadoop 及其 mapreduce 框架提供，这个输入格式能读取用换行符换行的文本文件。每读取一行都会调用一次指定的 mapper类的 map() 方法，这将处罚 ImportMapper.map() 函数
ImportMapper 类定义了两个方法，这两个方法重载了父类 Mapper，方法名均相同。
重载中存在的问题
强烈推荐重载添加 @Override 注释，这样错误的标签在编译时会被检查出来，否则，隐式的 map() 与 reduce() 方法会被调用，并实现恒等功能。例如，以下 reduce() 方法：
	public void reduce(Writable key,Iterator<Writable) values,
	Context context) throws IOException,InterruptedException{}
上述方法看起来正确，实则不然，其实并没有重载 Reducer 类的 reduce() 方法，而只是定义了 reduce() 方法的一个多态，MapReduce 框架会忽略这个方法，并执行由 Reducer 类提供的默认实现。
究其原因，实际应该标记的方法时：
	protected void reduce(KEYIN key,Iterable<VALUEIN> values,\
	Context context) throws IOException,InterruptedException
这是常见错误，在这里 Iterable 被错误地替换成了 Iteratore 类，在任务执行发生奇怪行为之前，在代码中重载的地方添加 @Override 注释能够帮助用户在编译的时候抛出错误（用户最），将注释添加到方法前面。
Importmappe 类的重载的 setup() 方法只会在框架初始化该类时调用一次，在这个例子中，它主要被用于将指定的列信息解析为列族和列限定符。
这个类的 map() 才是执行实际工作的实体，它会被输入的文本文件中的每一行调用，每一行都包含一个 jSON 格式的记录，这段代码会使用一行数据的 MD5 散列
值来创建一个 hBase 行键，然后使用 HBase 提供的名为 data:json 的列存储一行的内容。
这个例子通过 tableOutputFormat 类使用了隐式的写缓冲区，调用 context.write() 方法时，该方法内部会传入给定的 Put 实例并调用 table.put()，在作业结束前， TableOutputFormat 会主动调用flushCommits() 以保存仍驻留在写缓冲区的数据。
map() 方法可以通过写 Put 实例来存储输入数据，用户也可以通过写 Delete 实例来删除目标表中的数据，这就是设置作业输出键的类型为 Writable 接口，而并非显示的 Put 类的原因，
TableOutputFormat 当前仅能处理 Put 与 Delete 实例，将消息设置为除 Put 或 Delete 之外的实例都会导致抛出一个IOException 异常。
由于数据时存储了排序表中的，并且每行数据都拥有唯一的行键，用户可以在流程中避免更消耗的 sort、shuffle 和 reduce 阶段。

7.2.3 数据源
将数据导入到表中后，我们可以使用其中包含的数据来解析 JSON 记录，并从中提取信息，这里完全使用了 TableINputFormat 类，恰好对应了 TableOutputFormat。在 mapreduce 处理过程中，它以一张表为输入，使用了已提供的 InputFormat 类。
